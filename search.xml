<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PAPER READING&quot;2019-Wang-Self-Supervised-Spatio-Temporal-Representation-Learning-for-Videos-by-Predicting-Motion-and-Appearance-Statistics&quot;</title>
    <url>/2021/01/04/2019-Wang-Self-Supervised-Spatio-Temporal-Representation-Learning-for-Videos-by-Predicting-Motion-and-Appearance-Statistics/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><p>Self-Supervised Spatio-temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics</p>
<p><a href="https://laura-wang.github.io/" target="_blank" rel="noopener">Jiangliu Wang</a>, <a href="https://jianbojiao.com/" target="_blank" rel="noopener">Jianbo Jiao</a>, <a href="https://sites.google.com/site/linchaobao/" target="_blank" rel="noopener">Linchao Bao</a>, <a href="http://www.shengfenghe.com/" target="_blank" rel="noopener">Shengfeng He</a>, Yunhui Liu, <a href="http://www.ee.columbia.edu/~wliu/" target="_blank" rel="noopener">Wei Liu</a></p>
<p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019</p>
<p><a href="https://jianbojiao.com/bibs/cvpr_VidMAS.bib" target="_blank" rel="noopener">[BibTeX]</a> <a href="https://github.com/laura-wang/video_repres_mas" target="_blank" rel="noopener">[Code]</a></p>
<p>题目：通过运动统计和外观统计的预测，对视频进行自我监督时空表征学习<br>CVPR2019<br>港中文PHD, 实习腾讯AI LAB<br>个人web：<a href="https://laura-wang.github.io/" target="_blank" rel="noopener">Jiangliu Wang</a><br><a href="https://github.com/laura-wang/video_repres_mas" target="_blank" rel="noopener">[Code]</a> 有pytorch版</p>
<a id="more"></a>
<hr>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>行为识别</category>
      </categories>
      <tags>
        <tag>行为识别</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING 2020_Chen_Guided Dual Networks for Single Image Super-Resolution</title>
    <url>/2020/08/24/2020-Chen-Guided-Dual-Networks/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@article&#123;chenGuidedDualNetworks2020,</span><br><span class="line">  title = &#123;Guided &#123;&#123;Dual Networks&#125;&#125; <span class="keyword">for</span> &#123;&#123;Single Image Super&#125;&#125;-&#123;&#123;Resolution&#125;&#125;&#125;,</span><br><span class="line">  author = &#123;Chen, Wenhui and Liu, Chuangchuang and Yan, Yitong and Jin, Longcun and Sun, Xianfang and Peng, Xinyi&#125;,</span><br><span class="line">  year = &#123;2020&#125;,</span><br><span class="line">  volume = &#123;8&#125;,</span><br><span class="line">  pages = &#123;93608--93620&#125;,</span><br><span class="line">  issn = &#123;2169-3536&#125;,</span><br><span class="line">  doi = &#123;10.1109/ACCESS.2020.2995175&#125;,</span><br><span class="line">  journal = &#123;IEEE Access&#125;</span><br></pre></td></tr></table></figure>
<p>题目：引导式双路网络实现单图像超分辨率<br>Access  Q2/中科院2区<br>M.S.degree<br>指导老师： <a href="http://www2.scut.edu.cn/sse/2018/0614/c16789a270675/page.htm" target="_blank" rel="noopener">金龙存</a> 华工软件学院<br>group web：无<br>code：<a href="https://github.com/wenchen4321/GDSR" target="_blank" rel="noopener">https://github.com/wenchen4321/GDSR</a></p>
<p><strong>摘要</strong>：面向PSNR的超分辨率（SR）方法追求较高的重建精度，但会产生过度平滑的结果并丢失大量的高频细节。基于GAN的SR方法旨在生成更多逼真的图像，但是幻觉的细节通常会伴随着令人不满意的伪像和噪点。为了解决这些问题，我们提出了一种<strong>引导式双路超分辨率网络（GDSR）</strong>，该网络利用面向PSNR的方法和基于GAN的方法的优势，在重建精度和感知质量之间取得良好的平衡。具体来说，我们的网络包含<strong>两个分支</strong>，其中一个训练以提取全局信息，而另一个则专注于详细信息。通过这种方式，我们的网络可以生成同时具有高精度和令人满意的视觉质量的SR图像。<em>为了获得更多的高频特征，我们使用从低频分支中提取的全局特征来指导高频分支的训练。</em>此外，我们的方法利用掩模网络来自适应地恢复最终的超分辨图像。在几个标准基准上的广泛实验表明，与最新方法相比，我们提出的方法具有更好的性能。GDSR的开源代码和结果：<a href="https://github.com/wenchen4321/GDSR" target="_blank" rel="noopener">https://github.com/wenchen4321/GDSR</a></p>
<a id="more"></a>
<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>现存双路方法都是以PSNR为导向的，如DualCNN[16]和Dual-way SR[17]。我们的两路则一条PSNR导向，一条基于GAN。<br>受双路跳跃网络[18]（用于目标分类）的启发，我们提出GDSR。两个分支：高频分支（HFB）和低频分支（LFB）。 HFB采用对抗损失，旨在提取高频特征并使SR图像包含更多的详细信息；LFB经过MSE损失训练，以提取全局信息。与双路跳跃网络类似，我们采用自上而下的全局指导机制来指导HFB。简而言之，该指南将LFB的高级全局信息提供给HFB的相应低级功能处理模块。<br>此外，我们使用掩码网络来生成注意力掩码，以加权LFB和HFB的输出，自适应地恢复最终的超分辨图像。<br>创新点：</p>
<ol>
<li>通过整合基于GAN和面向PNSR的方法，提出了一种左右非对称的超分辨率网络，以提高SR图像质量。</li>
<li>我们采用了自上而下的全局指导，以提供从低频分支到高频分支的高级全局特征，以生成详细信息。</li>
<li>该方法在多个基准上均达到了SOTA的性能</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="SR"><a href="#SR" class="headerlink" title="SR"></a>SR</h2><p>ESRGAN<br>EEGAN [39]提出了一个基于GAN的边缘增强网络，该网络具有两个子网：一个基于GAN的超密集子网和一个基于CNN的边缘增强子网。然而，EEGAN专为卫星图像SR重建而设计，其中基于CNN的边缘增强子网用于从卫星图像中提取边缘的特殊特征。 </p>
<h2 id="双路跳跃网络"><a href="#双路跳跃网络" class="headerlink" title="双路跳跃网络"></a>双路跳跃网络</h2><p>双路跳跃网络[18]是一个左右非对称跳跃网络。一个分支用于细粒度级别分类，它模拟了处理空间高频信息的左半球；另一支用于粗分类，它模拟了处理空间低频信息的右半球机制。自上而下的引导识别：该引导将高级信息从粗分支馈送到细粒度分支的较低级别的视觉处理模块。<br>受此启发，我们设计了高频分支来模拟左半球处理机制，并设计了非对称的低频分支来处理右半球机制。此外，我们还设计了一个面具网络来模拟小脑的功能，它涉及平衡和运动控制。（这“类脑”有点神奇的感觉）</p>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p><img src="https://i.loli.net/2020/08/24/hniq9bDGaERgMVp.png" alt="image.png"><br>如图，三个关键组件组成：</p>
<ol>
<li>左右非对称SR网络；（两个分支组成：高频分支（HFB）和低频分支（LFB））</li>
<li>全局引导机制；将引导全局特征映射，从LFB到HFB的低级模块的全局特征映射，帮助HFB生成更多的高频细节。</li>
<li>掩码网络。自适应地重构来自LFB和HFB的最终输出，以提高SR图像的感知质量和重构精度。<h2 id="左右非对称"><a href="#左右非对称" class="headerlink" title="左右非对称"></a>左右非对称</h2>HFB用于恢复详细信息，而LFB用于重建全局信息。<h3 id="共享模块shared-module-SM"><a href="#共享模块shared-module-SM" class="headerlink" title="共享模块shared module (SM)"></a>共享模块shared module (SM)</h3>SM，包含S个RRDB块，由低频分支，高频分支和掩码网络共享，可以有效地提取浅层特征图并减少参数。<h3 id="低频分支LFB"><a href="#低频分支LFB" class="headerlink" title="低频分支LFB"></a>低频分支LFB</h3>由MSE loss训练，L个RRDB块<h3 id="高频分支HFB"><a href="#高频分支HFB" class="headerlink" title="高频分支HFB"></a>高频分支HFB</h3>包含生成网络G和判别网络D。G有H个RRDB块；D与ESRGAN类似，采用Ragan。<script type="math/tex; mode=display">L_1$$+$$L_percep$$+$$L_adv</script><img src="https://i.loli.net/2020/08/24/WP5gzkDa6ftEn3w.png" alt="image.png"><br><img src="https://i.loli.net/2020/08/24/TSfCmEMXbV2K45R.png" alt="image.png"><br><img src="https://i.loli.net/2020/08/24/V25IsUyc1BLlkdo.png" alt="image.png"></li>
</ol>
<h2 id="全局引导机制"><a href="#全局引导机制" class="headerlink" title="全局引导机制"></a>全局引导机制</h2><p>LSF低空间频率 双路跳跃网络[18]的启发，我们认为LFB能指导HFB使用输入的全局上下文特征来恢复更多详细信息。从全局级别注入反馈信息可能对细粒度的重构很有帮助。<br>the output feature maps of the l-th RRDB in the LFB <em>concatenated </em>into the input feature maps of the h-th RRDB in the HFB. </p>
<h2 id="掩码网络"><a href="#掩码网络" class="headerlink" title="掩码网络"></a>掩码网络</h2><p>为了使SR图专注于高频细节，我们嵌入注意力机制。M个RRDB块。<br>非在最终输出时融合，而在重建过程中融合特征图。</p>
<h1 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h1><h2 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h2><p>DIV2K数据集，分800、100和100张图像，分别用于训练，验证和测试。<br>测试：Set5, Set14, BSD100, Urban100, Manga109. LR图像由Bicubic降采样得到。<br>以PI和RMSE作为评价尺度。<br><img src="https://i.loli.net/2020/08/24/jyPner7wvKIqDQa.png" alt="image.png"><br>硬件：2块2080Ti GPU</p>
<h2 id="定量比较"><a href="#定量比较" class="headerlink" title="定量比较"></a>定量比较</h2><p>在所有基于GAN的方法（包括SRGAN [33]，EnhancedNet [34]，ESRGAN [15]，RankSRGAN [40]）中，我们的GDSR均具有最低的RMSE损耗和相对较低的PI值，并且可以生成具有更好的感知质量和相对较高的重建精度的SR图像</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1"><a href="#1" class="headerlink" title="1. "></a>1. </h2><h1 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h1><ol>
<li></li>
<li></li>
<li>计算机学术会议分级列表</li>
</ol>
<p>根据网络资料整理的计算机领域国际学术会议分级列表<a href="http://idc.hust.edu.cn/~rxli/csrank.htm以及国际学术期刊分级列表http://idc.hust.edu.cn/~rxli/csjrank.htm" target="_blank" rel="noopener">http://idc.hust.edu.cn/~rxli/csrank.htm以及国际学术期刊分级列表http://idc.hust.edu.cn/~rxli/csjrank.htm</a></p>
<p> <a href="https://www.cnblogs.com/bnuvincent/p/6809353.html" target="_blank" rel="noopener">https://www.cnblogs.com/bnuvincent/p/6809353.html</a></p>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>Super resolution</category>
      </categories>
      <tags>
        <tag>GDSR</tag>
        <tag>GAN</tag>
        <tag>Super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title>Note-use-ubuntu</title>
    <url>/2022/01/27/Note-use-ubuntu/</url>
    <content><![CDATA[<!-- ---
title: PAPER READING"Note-use-ubuntu"
mathjax: false
categories:
  - Ubuntu
tags:
  - 使用技巧
  - Ubuntu
date: 2021-07-01 21:52:33
--- -->
<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><p>&emsp;&emsp;记录一些日常使用的终端命令，便查。</p>
<a id="more"></a>
<h1 id="Linux通用"><a href="#Linux通用" class="headerlink" title="Linux通用"></a>Linux通用</h1><h2 id="文件与目录管理"><a href="#文件与目录管理" class="headerlink" title="文件与目录管理"></a>文件与目录管理</h2><p> <a href="https://blog.csdn.net/QFJIZHI/article/details/103550371" target="_blank" rel="noopener">跳转CSDN</a></p>
<h2 id="比较两个目录差异"><a href="#比较两个目录差异" class="headerlink" title="比较两个目录差异"></a><a href="https://baijiahao.baidu.com/s?id=1717681125622487958&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">比较两个目录差异</a></h2><p>diff<br>给定两个目录，如何找出哪些文件因内容不同</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diff --brief --recursive dir1/ dir2/</span><br><span class="line"><span class="comment"># --brief仅显示有无差异</span></span><br></pre></td></tr></table></figure>
<p>或者使用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diff -qr dir1/ dir2/</span><br><span class="line"><span class="comment"># -q 仅显示有无差异，不显示详细的信息</span></span><br><span class="line"><span class="comment"># -r 比较子目录中的文件</span></span><br></pre></td></tr></table></figure></p>
<h2 id="文件压缩和解压"><a href="#文件压缩和解压" class="headerlink" title="文件压缩和解压"></a>文件压缩和解压</h2><p>打包成tar.gz格式压缩包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zcvf renwolesshel.tar.gz /renwolesshel</span><br></pre></td></tr></table></figure></p>
<p>解压tar.gz格式压缩包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar zxvf renwolesshel.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>打包成tar.bz2格式压缩包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -jcvf renwolesshel.tar.bz2 /renwolesshel</span><br></pre></td></tr></table></figure></p>
<p>解压tar.bz2格式的压缩包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar jxvf renwolesshel.tar.bz2</span><br></pre></td></tr></table></figure></p>
<p>压缩成zip格式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zip -q -r renwolesshel.zip renwolesshel/</span><br></pre></td></tr></table></figure></p>
<p>解压zip格式的压缩包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">unzip renwolesshel.zip</span><br></pre></td></tr></table></figure></p>
<h2 id="Ubuntu查找文件或文件夹"><a href="#Ubuntu查找文件或文件夹" class="headerlink" title="Ubuntu查找文件或文件夹"></a>Ubuntu查找文件或文件夹</h2><blockquote>
<p>参见 <a href="https://blog.csdn.net/sunmingyang1987/article/details/107668149" target="_blank" rel="noopener">https://blog.csdn.net/sunmingyang1987/article/details/107668149</a></p>
</blockquote>
<h3 id="找文件"><a href="#找文件" class="headerlink" title="找文件"></a>找文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">locate 目录名/sh 		<span class="comment">#查找目录名下的sh开头文件</span></span><br><span class="line">locate -i 目录名/sh 		<span class="comment">#-i: 忽略大小写sh</span></span><br></pre></td></tr></table></figure>
<h3 id="找文件夹"><a href="#找文件夹" class="headerlink" title="找文件夹"></a>找文件夹</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find [directory] -name filename 		<span class="comment">#查找目录名下的filename文件夹</span></span><br></pre></td></tr></table></figure>
<p>加入额外的搜索条件,根据文件大小和文件名来搜索:让我们查找所有文件名匹配通配符模式“*.JPG”和文件大小大于 1M 的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find / -<span class="built_in">type</span>  f   -name   <span class="string">"*.JPG"</span>   -size   +1M</span><br></pre></td></tr></table></figure></p>
<h3 id="查看文件夹大小-more"><a href="#查看文件夹大小-more" class="headerlink" title="查看文件夹大小 more"></a>查看文件夹大小 <a href="https://www.cnblogs.com/Sungeek/p/11661554.html" target="_blank" rel="noopener">more</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">du -sh</span><br></pre></td></tr></table></figure>
<h2 id="查看磁盘容量"><a href="#查看磁盘容量" class="headerlink" title="查看磁盘容量"></a>查看磁盘容量</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure>
<h2 id="GCC版本切换"><a href="#GCC版本切换" class="headerlink" title="GCC版本切换"></a><a href="https://blog.csdn.net/QFJIZHI/article/details/103372864" target="_blank" rel="noopener">GCC版本切换</a></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config gcc</span><br></pre></td></tr></table></figure>
<h2 id="查找大文件（夹）"><a href="#查找大文件（夹）" class="headerlink" title="查找大文件（夹）"></a>查找大文件（夹）</h2><h3 id="找大文件夹"><a href="#找大文件夹" class="headerlink" title="找大文件夹"></a>找大文件夹</h3><ol>
<li>方法一<br>（所有目录）<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># du -am | sort -nr | head -n 15  # 运行时间较长</span></span><br><span class="line">1215060 .</span><br><span class="line">369830  ./data</span><br><span class="line">124092  ./myfolder</span><br><span class="line">119186  ./data/volleyball</span><br><span class="line">107788  ./LJ</span><br><span class="line">77811   ./rx</span><br><span class="line">73689   ./anaconda</span><br><span class="line">69774   ./.PyCharm2018.2</span><br><span class="line">69770   ./.PyCharm2018.2/system</span><br><span class="line">69069   ./.PyCharm2018.2/system/caches</span><br><span class="line">68741   ./.PyCharm2018.2/system/caches/content.dat.storageData</span><br><span class="line">66731   ./JRY</span><br><span class="line">59862   ./data/volleyball/videos</span><br><span class="line">59151   ./data/volleyball/volleyball.zip</span><br><span class="line">58626   ./anaconda/pkgs</span><br></pre></td></tr></table></figure></li>
<li>方法二<br>（当前目录）<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># du -s * | sort -nr | head -n 15  # 运行时间较短</span></span><br><span class="line">``` </span><br><span class="line">3. 方法三</span><br><span class="line">（优选 当前目录，显示单位为Gb/Mb）</span><br><span class="line">``` bash</span><br><span class="line"><span class="comment"># du -hsx * | sort -rh | head -n 15  # 运行时间较短</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 找大文件</span></span><br><span class="line">``` bash</span><br><span class="line"><span class="comment"># find . -type f -printf "%k %p\n" | sort -rn | head -n 15</span></span><br><span class="line">70390696 ./.PyCharm2018.2/system/caches/content.dat.storageData</span><br><span class="line">60569632 ./data/volleyball/volleyball.zip</span><br><span class="line">**22718764 ./data/data/ucf101_tvl1_flow.zip**</span><br><span class="line">15956596 ./Conrad/1/SSTran-DSGG/ActionGenome-master/tools/dataset/ag/videos/Charades_v1_480.zip</span><br><span class="line">**9437188 ./tan/ucf101_tvl1_flow.zip.001**</span><br><span class="line">**9437188 ./data/data/ucf101_tvl1_flow.zip.002**</span><br><span class="line">7282032 ./rx/data/tiered_imagenet/compacted_datasets/train_images_png.pkl</span><br><span class="line">6954056 ./caffe/examples/UTD_CRC.zip</span><br><span class="line">**6770488 ./tan/five-video-classification-methods-master/data/UCF101.rar**</span><br><span class="line">5720616 ./space/Super-Resolution-Datasets/DIV2K2017.zip</span><br><span class="line">4193284 ./data/g</span><br><span class="line">3956776 ./data/BOBO/RefNet/data/ADE20K_2016_07_26.zip</span><br><span class="line">3956776 ./data/BOBO/AttentionSegModel/data/ADE20K_2016_07_26.zip</span><br><span class="line">3869568 ./caffe/models/model-zoo.zip</span><br><span class="line">**3844396 ./data/data/ucf101_tvl1_flow.zip.003**</span><br></pre></td></tr></table></figure>
</li>
</ol>
<!-- #数据下载
Google Drive direct download 
https://github.com/circulosmeos/gdown.pl
./gdown.pl 'gdrive file url' ['desired file name']
./gdown.pl https://drive.google.com/file/d/1DtPo-WP-hjaOwsbj6ZxTtOo_7R_4TKRG/view /home/yangyin/data/datasets/cityscapes/refinement_final_v0.zip
curl -H "Authorization: Bearer ya29.a0ARrdaM_goUMCOrz3yrQB10myg2vHw5gS5PbbtPSQP2vl97sgg8TzFmuD3qysWLzm-bpO7oyATXE33damDUcsks8Lc9Kh8WxL6iu-gR8XrkW2LFF-T3KPIyZx8eAizHKLhb-FsVhjgFh6txNNaYkgdIidalRt" https://www.googleapis.com/drive/v3/files/1DtPo-WP-hjaOwsbj6ZxTtOo_7R_4TKRG?alt=media -o refinement_final_v0.zip

Invoke-RestMethod -Uri https://www.googleapis.com/drive/v3/files/1qiMFxUn-AYaBCgv03_wPZGdJUKaMW6SH?alt=media -Method Get -Headers @{"Authorization"="Bearer ya29.a0ARrdaM9vAbT982bBbfNfmBeuGlSaGk8EZt3avTQ7EXJwDY6rK8dsmjtGqPtzJinq7V8j7XttelPs4a-MYwm8aUdFZTqqFwGRPItP9NwGs25EuqEKUXIHM5BF46TctQPj0CzANPM4Bv9knqyekZNs95FxrVrb"} -OutFile ocrnet.HRNet_industrious-chicken.pth -->
<h1 id="显卡驱动cuda及软连接"><a href="#显卡驱动cuda及软连接" class="headerlink" title="显卡驱动cuda及软连接"></a>显卡驱动cuda及软连接</h1><blockquote>
<p>参见 <a href="https://blog.csdn.net/qq_37424778/article/details/115293430" target="_blank" rel="noopener">https://blog.csdn.net/qq_37424778/article/details/115293430</a></p>
</blockquote>
<h2 id="查看-bashrc中环境配置"><a href="#查看-bashrc中环境配置" class="headerlink" title="查看.bashrc中环境配置"></a>查看.bashrc中环境配置</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="查看编译使用的cuda环境"><a href="#查看编译使用的cuda环境" class="headerlink" title="查看编译使用的cuda环境"></a>查看编译使用的cuda环境</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure>
<p>启动/关闭vncserver<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vncserver :1</span><br><span class="line">vncserver -<span class="built_in">kill</span> :1</span><br></pre></td></tr></table></figure></p>
<h1 id="CONDA使用"><a href="#CONDA使用" class="headerlink" title="CONDA使用"></a>CONDA使用</h1><blockquote>
<p> <a href="https://blog.csdn.net/QFJIZHI/article/details/103963782" target="_blank" rel="noopener">conda 常用指令总结</a></p>
</blockquote>
<h2 id="国内镜像源"><a href="#国内镜像源" class="headerlink" title="国内镜像源"></a>国内镜像源</h2><blockquote>
<p>添加通道 <a href="https://mirror.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">more info</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure>
<p>查看channels<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ~/.condarc</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda create -n torch160 python=3.7 -y</span><br></pre></td></tr></table></figure>
<p>-y ：表静默yes</p>
<h2 id="安装torch-1-6-0"><a href="#安装torch-1-6-0" class="headerlink" title="安装torch 1.6.0"></a>安装torch 1.6.0</h2><p>for cuda10.1 <a href="https://pytorch.org/get-started/previous-versions/" target="_blank" rel="noopener">more info</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.1</span><br></pre></td></tr></table></figure></p>
<h1 id="apex包-amp-inplace-abn"><a href="#apex包-amp-inplace-abn" class="headerlink" title="apex包 &amp; inplace-abn"></a>apex包 &amp; inplace-abn</h1><p>以前需要编译安装，现conda已加入此包。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install -c conda-forge nvidia-apex</span><br></pre></td></tr></table></figure><br><a href="https://github.com/mapillary/inplace_abn" target="_blank" rel="noopener">inplace-abn</a>:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># PyTorch &gt;=1.1  &amp; CUDA &gt;= 10.0</span></span><br><span class="line">pip install inplace-abn</span><br></pre></td></tr></table></figure></p>
<h1 id="MMCV等包"><a href="#MMCV等包" class="headerlink" title="MMCV等包"></a>MMCV等包</h1><p>MMCV<a href="https://github.com/open-mmlab/mmcv" target="_blank" rel="noopener">github</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html</span><br></pre></td></tr></table></figure></p>
<h1 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h1><h2 id="nohup命令"><a href="#nohup命令" class="headerlink" title="nohup命令"></a>nohup命令</h2><blockquote>
<p>参见<a href="https://www.runoob.com/linux/linux-comm-nohup.html" target="_blank" rel="noopener">https://www.runoob.com/linux/linux-comm-nohup.html</a></p>
</blockquote>
<p>应对网络异常中断，使程序执行不受干扰。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nohup python -u train.py &gt; run_lr_02.log 2&gt;&amp;1 &amp;</span><br><span class="line">nohup python -u train.py &gt; run_r3.log &amp;</span><br><span class="line"><span class="comment">#CUDA_VISIBLE_DEVICES=1, python train.py</span></span><br></pre></td></tr></table></figure></p>
<h1 id="问题速查"><a href="#问题速查" class="headerlink" title="问题速查"></a>问题速查</h1><h2 id="连显示器无显示"><a href="#连显示器无显示" class="headerlink" title="连显示器无显示"></a>连显示器无显示</h2><p>因：nvidia-smi，可看到是输出关闭了<br>解：关闭显示服务，重启。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service lightdm restart</span><br></pre></td></tr></table></figure></p>
]]></content>
  </entry>
  <entry>
    <title>PAPER READING&quot;2020_Wei_Unsupervised Real-world Image Super Resolution via Domain-distance Aware&quot;</title>
    <url>/2021/04/10/2020-Wei-Unsupervised-Real-world-Image-Super-Resolution-via-Domain-distance-Aware/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Wei, Yunxuan, Shuhang Gu, Yawei Li, and Longcun Jin. </span><br><span class="line">“Unsupervised Real-World Image Super Resolution via Domain-Distance Aware Training.”</span><br><span class="line">ArXiv:2004.01178 [Cs], April 2, 2020. http://arxiv.org/abs/2004.01178.</span><br><span class="line">CVPR-2021</span><br></pre></td></tr></table></figure>
<p>题目：通过 域距离感知训练 实现 <em>无监督的</em> 真实世界图像超分辨率<br>CVPR-2021 项目页已提<br>code: <a href="https://github.com/ShuhangGu/DASR" target="_blank" rel="noopener">https://github.com/ShuhangGu/DASR</a><br>blog: 一作Yunxuan Wei （华南理工学生，导师<a href="http://www2.scut.edu.cn/sse/2018/0614/c23650a339994/page.htm" target="_blank" rel="noopener">金龙存</a> , 悉尼大学交流生）<br>&emsp;&emsp;二作(<a href="https://shuhanggu.github.io/" target="_blank" rel="noopener">顾舒航</a>, 悉尼大学AP，博后 at ETH Zurich, under the supervision of  Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" target="_blank" rel="noopener">Luc Van Gool</a>. )<a href="https://sites.google.com/site/shuhanggu/" target="_blank" rel="noopener">https://sites.google.com/site/shuhanggu/</a></p>
<p><strong>摘要</strong>：<br>&emsp;&emsp;无监督超分。 现有方法的原理在于增强未配对数据，即首先生成 <em>与真实世界域中LR图像<script type="math/tex">y ^ { r }</script>-（HR）图像<script type="math/tex">x ^ { r }</script>相对应的</em> <strong>合成</strong>低分辨率（LR）图像<script type="math/tex">y ^ { g }</script>；然后利用伪图像对<script type="math/tex">\{ y ^ { g } , x ^ { r } \}</script> 进行监督训练。不幸的是，由于图像迁移本身是一项极具挑战性的任务，因此这些方法的SR性能受到生成的合成LR图像和真实LR图像之间的域间差距的严重限制。<br>在本文中，我们提出了一种无监督的用于现实世界图像SR的新颖的 <strong>域距离感知超分辨率（DASR）</strong>方法。 通过我们的域间差距感知训练和域距离加权监督策略，解决训练数据（<script type="math/tex">y ^ { g }</script>）和测试数据（<script type="math/tex">y ^ { r }</script>）之间的领间差距。 域间差距感知训练充分利用目标域（real）的实际数据；域距离加权监督则更合理使用了有标签的源域（合成）数据。 该方法在合成数据集和真实数据集上得到了验证，实验结果表明，在生成具有更真实自然纹理的SR输出时，DASR始终优于最先进的无监督SR方法。</p>
<a id="more"></a>
<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>&emsp;&emsp;然而，现有的无监督SR方法[17,7,38]在SR网络的训练过程中忽略了<script type="math/tex">y ^ { g }</script>和<script type="math/tex">y ^ { r }</script>之间的域差距。在图1中，我们给出了一个可视化的例子来显示：合成LR与真实LR图像之间存在领域间差距。尽管训练有素的下采样网络能够生成比双三次下采样的图像更好的LR图像，但合成LR与真实LR之间仍然存在域间隙。</p>
<p>&emsp;&emsp;在本文中，我们提出了一种用于现实世界中图像超分辨率的域距离感知超分辨率（DASR）框架。 与以前的非监督方法[7,38,17,24]不同，后者依靠伪对的生成进行监督训练，我们的DASR考虑了生成的图像和真实的LR图像之间的域间隙，即<script type="math/tex">y ^ { g }</script>和<script type="math/tex">y ^ { r }</script>， 并在域自适应机制下用这两个方法解决SR问题。<br><img src="https://i.loli.net/2021/04/10/P4G69vuoMw3tsEQ.png" alt="quicker_87c16169-e9a4-4a1f-8ae2-1739b6af2e60.png"></p>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p>主要贡献：两种训练策略（域间差距感知训练和域距离加权监督策略）。另外，采用了更好的下采样网络结构和更好的对抗损失（小波域中）。<br>系统框架如图<br><img src="https://i.loli.net/2021/04/10/zvZJb7owIXpuUfK.png" alt="quicker_7b0d17d7-a221-420f-9b98-35e32c783c57.png"></p>
<p>其中，<script type="math/tex">y ^ { b }</script>表示bicubic下采样后的LR。<br>我们遵循以前的最先进的方法[17,7,38]，并提出了一个两阶段的方法。 首先，我们训练下采样网络（DSN）从HR图像 生成贴近真实LR域的LR图像：<script type="math/tex">y _ { i } ^ { g } = D S N ( x _ { i } ^ { r } )</script>。 然后，我们利用生成的LR-HR对<script type="math/tex">\{ y _ { i } ^ { g } , x _ { i } ^ { r } \} _ { i  = 1 , \cdots , M}</script>用于训练超分网络（SRN）。</p>
<h2 id="下采样网络（DSN）"><a href="#下采样网络（DSN）" class="headerlink" title="下采样网络（DSN）"></a>下采样网络（DSN）</h2><p><img src="https://i.loli.net/2021/04/10/7XBflaKePtYFJI3.png" alt="quicker_864587e3-c8f2-493f-8bfc-6baefb57eed8.png"><br>图3（a）下采样网络（DSN）结构 （b）小波变换高频中的对抗损失</p>
<p><strong>结构</strong>：DSN利用23个残差块从HR图像中提取信息，每个残差块包含两个卷积层(内核大小为3*3，通道为64)，中间有一个ReLU激活。然后，采用双线性调整算子和两个卷积层来降低特征的空间分辨率，并将特征投影回图像域。</p>
<p>LOSS:</p>
<script type="math/tex; mode=display">\mathcal{L}_{\text {con }}=\mathbb{E}_{x^{r}}\left\|y_{i}^{b}-D S N\left(x_{i}^{r}\right)\right\|_{1}, \quad \mathcal{L}_{\text {per }}=\mathbb{E}_{x^{r}}\left\|\phi\left(y_{i}^{b}\right)-\phi\left(D S N\left(x_{i}^{r}\right)\right)\right\|_{1}</script><p>perceptual loss 中<script type="math/tex">\phi ( \cdot )</script> 表示VGG 特征提取器（同ESRGAN）。<br><img src="https://i.loli.net/2021/04/10/QwdCkEHceXvVplJ.png" alt="quicker_7751f111-b6f0-40a6-ad46-193b6d9871f2.png"></p>
<p>与FSSR不同(处理高频特征的对抗损失)：本文使用Haar小波变换来提取更多有用的高频分量。具体地，将由Haar小波变换分解的四个子带分别表示为LL，LH，HL和HH，我们将LH，HL和HH分量堆叠为鉴别器的输入。 与FSSR中使用的高频提取器相比[17]，我们基于小波的提取器还利用方向信息更好地表征图像细节。同时，网络因维度降低，减小了GAN训练的难度。<br>总loss：</p>
<script type="math/tex; mode=display">L _ { DSN }  = \alpha L _ { c o n }+ \beta L _ { p e r }  + \gamma L _ { a d v } ^ { G }</script><h2 id="SRN的域间差距感知训练"><a href="#SRN的域间差距感知训练" class="headerlink" title="SRN的域间差距感知训练"></a>SRN的域间差距感知训练</h2><p><img src="https://i.loli.net/2021/04/10/BMPbOoV3rf7naIF.png" alt="quicker_90dc52b3-cb9c-4543-a471-ed06d9c1a09c.png"><br>Loss见原文</p>
<h1 id="合成数据集的实验结果与消融实验"><a href="#合成数据集的实验结果与消融实验" class="headerlink" title="合成数据集的实验结果与消融实验"></a>合成数据集的实验结果与消融实验</h1><h2 id="合成数据集"><a href="#合成数据集" class="headerlink" title="合成数据集"></a>合成数据集</h2><p>采用AIM dataset：used in the AIM Challenge on Real World SR at ICCV 2019 [39].<br>训练集：Flickr2K dataset-2650，DIV2K-800HR<br>验证集：100images，paired<br>结果见表3，可视化见图5。</p>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="数据图像对生成效果-更好的DSN"><a href="#数据图像对生成效果-更好的DSN" class="headerlink" title="数据图像对生成效果 更好的DSN"></a>数据图像对生成效果 更好的DSN</h3><p><img src="https://i.loli.net/2021/04/10/qdpYcw6GatNbVTv.png" alt="quicker_4b4bdc50-7452-4ee7-9e4a-dab2ba854e66.png"><br>Gaussian Blur Frequency Separation (GBFS), Wavelet Frequency Separation (WFS) and RGB indicate the model conducts adversarial training in different spaces: GBFS uses the residual between original and Gaussian blurred images to extract high frequency component, our WFS approach adopts Wavelet transform to obtain high frequency component, RGB means we introduce GAN loss directly on RGB images.</p>
<h3 id="各组件"><a href="#各组件" class="headerlink" title="各组件"></a>各组件</h3><p><img src="https://i.loli.net/2021/04/10/E8MmpOeVN5K2zvx.png" alt="quicker_27c2b219-07f7-4e9b-8c38-c86dff4a6055.png"></p>
<h1 id="真实世界数据集的实验"><a href="#真实世界数据集的实验" class="headerlink" title="真实世界数据集的实验"></a>真实世界数据集的实验</h1><h2 id="真实世界数据集"><a href="#真实世界数据集" class="headerlink" title="真实世界数据集"></a>真实世界数据集</h2><p>2个：RealSR [8] (@ ICCV2019)选200LR and CameraSR [9]（City100 @ CVPR2019）选100LR<br>训练时同合成的方法，加入了DIV2K-800HR</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://i.loli.net/2021/04/10/WfynIpcDXvUojMi.png" alt="quicker_b5b5d705-f9b7-4262-9c8f-0ec92cd932b1.png"></p>
<p><img src="https://i.loli.net/2021/04/10/VBepU9fsGah56c2.png" alt="quicker_710ca11d-4d3b-476f-915c-c2922d343d18.png"></p>
<p><img src="https://i.loli.net/2021/04/10/tJaZEXpN6GVi3Pv.png" alt="quicker_8d0dd79b-8dcb-4207-bae1-d8b79c2cbf7e.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>四问</p>
<pre><code>1. 作者想要完成什么（已经完成了什么）？
</code></pre><p>一种无监督的用于现实世界图像SR的新颖的 <strong>域距离感知超分辨率（DASR）</strong>方法。两种训练策略消除LR图像的合成域和真实域的间距。该文在FSSR基础上实现，采用了Haar小波变换 分离出高频信息，优化了对抗损失。</p>
<pre><code>2. 新方法的关键元素是什么？
</code></pre><p>两种训练策略消除LR图像的合成域和真实域的间距。该文在FSSR基础上实现，采用了Haar小波变换 分离出高频信息，优化了对抗损失</p>
<pre><code>3. 对我有用吗？
</code></pre><p>本文Real world SR 中的基于域适应的方法。  启发 风格迁移用于SR问题（数据集合成、损失LOSS）</p>
<pre><code>4. 想看他的哪些参考文献？
</code></pre><p>暂无</p>
<p>其他疑问：</p>
<ol>
<li>未明白 作者的表述pre-trained ESRGAN (P.T. ESRGAN)和supervisely trained ESRGAN (S.T. ESRGAN)</li>
<li>SRN结构, 估计得从代码上看。可能同FSSR吧</li>
<li>补充材料未看</li>
</ol>
<hr>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>Super resolution</category>
      </categories>
      <tags>
        <tag>Super resolution</tag>
        <tag>real world</tag>
        <tag>DASR</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING 2020_Lei_Coupled Adversarial Training for Remote Sensing Image Super-Resolution</title>
    <url>/2020/08/17/2020_Lei_Coupled-Adversarial-Training/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">article&#123;leiCoupledAdversarialTraining2020,</span><br><span class="line">  title = &#123;Coupled &#123;&#123;Adversarial Training&#125;&#125; <span class="keyword">for</span> &#123;&#123;Remote Sensing Image Super&#125;&#125;-&#123;&#123;Resolution&#125;&#125;&#125;,</span><br><span class="line">  author = &#123;Lei, Sen and Shi, Zhenwei and Zou, Zhengxia&#125;,</span><br><span class="line">  year = &#123;2020&#125;,</span><br><span class="line">  month = may,</span><br><span class="line">  volume = &#123;58&#125;,</span><br><span class="line">  pages = &#123;3633--3643&#125;,</span><br><span class="line">  issn = &#123;1558-0644&#125;,</span><br><span class="line">  doi = &#123;10.1109/TGRS.2019.2959020&#125;,</span><br><span class="line">  journal = &#123;IEEE Transactions on Geoscience and Remote Sensing&#125;,</span><br><span class="line">  number = &#123;5&#125;</span><br></pre></td></tr></table></figure>
<p>题目：结合对抗训练进行遥感影像超分辨率<br>JCRQ1/中科院1区<br>指导老师：史振威，北航宇航学院<br>group web：<a href="http://levir.buaa.edu.cn/" target="_blank" rel="noopener">http://levir.buaa.edu.cn/</a><br>无code。</p>
<p>摘要：<br>生成对抗网络（GAN）在最近的自然图像超分辨率任务中取得了巨大进展，其成功的关键是集成了一个鉴别器，该鉴别器经过训练可以对输入是真实的高分辨率（HR）图像还是生成的图像进行分类。可以说，学习强大的判别先验对于生成高质量图像至关重要。然而，通过广泛的统计分析，我们发现，遥感图像中的低频分量比自然图像更多，这可能会导致“判别器歧义”问题，即在处理这些低频分量多的图像时，辨别器将“困惑”于判断其输入是真实的还是非真实的，因此，生成的HR图像质量可能会受到严重影响。为了解决这个问题，我们提出了一种新颖的基于GAN的超分辨率算法，称为<strong>耦合鉴别GAN（CDGAN）</strong>，用于遥感图像。与以前的基于GAN的超分辨率模型不同，鉴别器一次获取一张图像；在该模型中，该鉴别器经过专门设计，可以获取一对图像：生成的图像及其HR GT，以便更好地区分输入。我们进一步介绍了<strong>双路径网络架构，随机门和耦合对抗损失</strong>，以更好地学习判别结果与输入样本对之间的对应关系。在两个公共数据集上的实验结果表明，与其他现有技术相比，我们的模型在视觉外观和局部细节方面都可以获得更准确的超分辨率结果。<em>我们的代码将公开提供。（暂时还没公布）</em></p>
<a id="more"></a>
<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>遥感图像使用GAN的“判别器歧义”问题：<br>遥感图像很难直接判别。e.g.沙漠和海滩的遥感图像，高频信息和局部细节很少，其HR和LR图相差无几。</p>
<blockquote>
<p>在遥感图像中，我们通过广泛的统计分析发现，与自然图像相比，例如沙漠和海滩地区，存在更多的平坦区域和更多的低频图像成分。</p>
<blockquote>
<p>However, in remote sensing images, we discoverthrough extensive statistical analysis that there are more flatregions and more low-frequency image components than thenatural images, e.g., the areas of the desert and beach.</p>
</blockquote>
</blockquote>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><h2 id="总结构"><a href="#总结构" class="headerlink" title="总结构"></a>总结构</h2><p>生成器借鉴了SRGAN、EDSR、ESRGAN的结构。总结构见图4（下图）和表I.<br><img src="https://i.loli.net/2020/08/21/rtEzkKGhOM1Je6x.png" alt="image.png"></p>
<p><strong>耦合判别器</strong>：<br>输入对（SR和GT),先过<strong>随机门</strong>（打乱输入对的顺序，并加顺序标签<script type="math/tex">d_z</script>bool值）。送入双路。<br><img src="https://i.loli.net/2020/08/21/iSeHvONXczCwYI1.png" alt="image.png"></p>
<p>判别器网络借鉴了DCGAN的结构（所有的pooling层使用步幅卷积，及使用LeakyReLU激活函数）。</p>
<h2 id="LOSS"><a href="#LOSS" class="headerlink" title="LOSS"></a>LOSS</h2><p>采用<script type="math/tex">L_{ads} + L_{content}</script><br>设置<script type="math/tex">\lambda = 10^4</script> ？？<br><img src="https://i.loli.net/2020/08/23/KeHhBpraq9JcXuz.png" alt="image.png"></p>
<h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>传统的两阶段，阶段一预训练仅采用均方差loss；阶段二完整loss，迭代训练。</p>
<h1 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>遥感图像：</p>
<ol>
<li>“UCMerced”<a href="http://www.graphnetcloud.cn/1-1" target="_blank" rel="noopener">中科链接</a> <a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html" target="_blank" rel="noopener">官方链接</a>[45] and “WHU-RS19”<a href="http://www.graphnetcloud.cn/1-22" target="_blank" rel="noopener">中科链接</a> [46]<br>40%用于训练，10%用于验证，剩余50%用于测试</li>
<li>自选 “高分2”数据集 ，作测试。无HR GT。</li>
</ol>
<h2 id="评估标准"><a href="#评估标准" class="headerlink" title="评估标准"></a>评估标准</h2><p>PSNR[Python]+perception index (PI)[Matlab]+ Learned Perceptual ImagePatch Similarity (LPIPS) [Python]<br>PI是无需GT的。</p>
<blockquote>
<p>The implementation of the evaluation metrics can be found in thefollowing websites. PSNR: <a href="https://github.com/scikit-image/scikit-image" target="_blank" rel="noopener">https://github.com/scikit-image/scikit-image</a>, PI:<a href="https://github.com/roimehrez/PIRM2018" target="_blank" rel="noopener">https://github.com/roimehrez/PIRM2018</a>, LPIPS: <a href="https://github.com/richzhang/PerceptualSimilarity" target="_blank" rel="noopener">https://github.com/richzhang/PerceptualSimilarity</a></p>
</blockquote>
<p>对于 “高分2”数据集（无HR GT），使用了另两个无需参考的评价指标NIQE和SSEQ。</p>
<h2 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h2><p><img src="https://i.loli.net/2020/08/23/36lWvQj9BfZHD5N.png" alt="image.png"></p>
<h2 id="与其他方法的比较"><a href="#与其他方法的比较" class="headerlink" title="与其他方法的比较"></a>与其他方法的比较</h2><p>在UCMerced集和WHU-RS19数据集上，与SRCNN [57]，LGCNet [16]，EDSR [32]，RCAN [58]和SRGAN [25]等一些超分辨率方法进行了比较。<br>图9展示了，在“高分2”数据集（无HR GT）上的比较，则没比较PSNR、PI、LPIPS，使用了另两个无需参考的评价指标NIQE和SSEQ。<br>表V中，我们报告了三个关于模型效率的指标，即模型参数，浮点运算数<script type="math/tex">{FLOP}^2</script>和推理时间（GPU或CPU模式下）。我们使用WHU-RS19数据集来计算FLOPs和推理时间。硬件1080Ti、i7-6700K、32GB RAM。应当指出的是，由于基于GAN的方法的推理时间是由其生成器确定的，因此我们在这里仅列出生成器的参数。如表V所示，尽管CDGAN的参数比SRCNN的大20倍，推理时间要少。（原文这SRCNN比得有点吃亏，应该和SRGAN比。效率方面的结果也是比SRGAN好的）</p>
<blockquote>
<p>The model parameters and FLOPs can be counted by the following repository: <a href="https://github.com/Lyken17/pytorch-OpCounter" target="_blank" rel="noopener">https://github.com/Lyken17/pytorch-OpCounter</a></p>
</blockquote>
<p><img src="https://i.loli.net/2020/08/23/MKnGzYV5ENA8Usd.png" alt="image.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>针对遥感特定应用缺点“判别器歧义”，<strong>采用成对输入的判别器</strong>，解决了这个问题，特别是遥感图像中的低频区域。</p>
<ol>
<li>为了实现，结构上设计了双路径网络架构，随机门。损失函数相应采用耦合对抗损失。</li>
</ol>
<hr>
<h1 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h1><ol>
<li>特定域的超分。将超分应用于遥感图像，对其分类计算了实验效果，在LPIPS上，所比较的方法中最优。</li>
<li>采用成对输入的判别器是创新点。但其中随机门的设计，没有理解作者的用途（原文：To learn better correspondence between the input pair and the discriminative outputs）</li>
<li>数据集（4：1：5）比例划分。验证和测试的区别在哪。<a href="https://blog.csdn.net/qq_43741312/article/details/96994243" target="_blank" rel="noopener">https://blog.csdn.net/qq_43741312/article/details/96994243</a></li>
<li>学到的：对于无GT的自然图，采用了NIQE和SSEQ评价，也可采用PI。</li>
</ol>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>Super resolution</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>Super resolution</tag>
        <tag>CDGAN</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING&quot;Adversarial-Self-Supervised-Learning-for-Semi-Supervised-3D-Action-Recognition&quot;</title>
    <url>/2020/12/30/Adversarial-Self-Supervised-Learning-for-Semi-Supervised-3D-Action-Recognition/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Si, Chenyang, Xuecheng Nie, Wei Wang, Liang Wang, Tieniu Tan, and Jiashi Feng. </span><br><span class="line"><span class="string">"Adversarial Self-Supervised Learning for Semi-Supervised 3D Action Recognition."</span> </span><br><span class="line">In European Conference on Computer Vision, pp. 35-51. Springer, Cham, 2020.</span><br></pre></td></tr></table></figure>
<p>题目：对抗式自我监督学习，实现半监督3D动作识别<br>ECCV2020<br>一作 司晨阳，中科院自动化所博士生，新加坡国立大学访问学生<br>指导老师：Prof. <a href="http://cripac.ia.ac.cn/en/EN/column/item80.shtml" target="_blank" rel="noopener">Tieniu Tan</a>, Prof. <a href="http://cripac.ia.ac.cn/en/EN/column/item125.shtml" target="_blank" rel="noopener">Liang Wang</a> and associate Prof. <a href="http://cripac.ia.ac.cn/en/EN/column/item118.shtml" target="_blank" rel="noopener">Wei Wang</a><br>web：<a href="http://chenyangsi.top/" target="_blank" rel="noopener">http://chenyangsi.top/</a><br>无code。作者开源代码少。<br><a id="more"></a><br><strong>摘要</strong>：<br>&emsp;&emsp;We consider the problem of semi-supervised <strong>3D action recognition</strong> which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. <strong>To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning.</strong> Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on <strong>NTU and N-UCLA datasets</strong>. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol>
<li>3D-Action-Recognition，即，基于骨架的行为识别</li>
<li>通过邻域关系结合了对抗学习和自监督</li>
<li>NTU RGB+D dataset 40个人、60个动作类别，56880个样本。三个摄像机在不同水平视角位置同时采集。</li>
</ol>
<hr>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>行为识别</category>
      </categories>
      <tags>
        <tag>行为识别</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING&quot;Few-shot-action-recognition-with-permutation-invariant-attention&quot;</title>
    <url>/2020/12/18/Few-shot-action-recognition-with-permutation-invariant-attention/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;Zhang2020FewShotAR,</span><br><span class="line">  title=&#123;Few-Shot Action Recognition with Permutation-Invariant Attention&#125;,</span><br><span class="line">  author=&#123;Hongguang Zhang and Liyong Zhang and Xiaojuan Qi and Hongdong Li and P. Torr and Piotr Koniusz&#125;,</span><br><span class="line">  booktitle=&#123;ECCV&#125;,</span><br><span class="line">  year=&#123;2020&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>题目：<br>2020ECCV</p>
<p><strong>摘要</strong>：<br>&emsp;&emsp;我们构建了一个C3D编码器的视频时空块，以捕获短距离的动作模式。这些编码块通过排列不变性池化聚合；使我们的方法具有强大的鲁棒性，能应对变化的动作长度和长期的时间依赖性，这些模式，即使在同一类别的剪辑中，不太可能重复。随后，合并的表征被组合成简单的关系描述符，用于编码所谓的查询和支持片段。最后，将关系描述符输入到比较器，以实现查询和支持片段之间的相似性学习。重要的是，为了在池化中重新计算块贡献的权重，我们利用了时空注意力模块和自我监督。在自然剪辑中(同类别)存在着时间分布的偏移——区分性时间动作热点的位置不同。因此，我们对片段的块进行置换，并将结果的注意区域与非置换片段的类似置换的注意区域进行比对，以训练不改变块(从而不改变长期热点)置换的注意机制。我们的方法优于目前最先进的HMDB51, UCF101,miniMIT数据集</p>
<a id="more"></a>
<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>&emsp;&emsp;对于少样本学习问题，尝试的方法有：元学习，通过关系学习进行的鲁棒特征表示法，基于梯度的，对数据集进行生成扩展。<br>相比其他的FSL FOR AR，我们更关注于<strong>鲁棒的关系/相似性特征</strong>，通过<strong>置换不变池化和注意力机制</strong>对短期和长期动作模式进行<strong>时空建模</strong>。<br>研究内容如下：</p>
<ol>
<li>表示具有区分性的短期和非重复性长期动作模式，以进行相关/相似性学习;<br>&emsp;&emsp;包含C3D卷积块的编码器可捕获有区分性的短期动作模式;然后，编码后的特征经过置换不变池化(permutation-invariant pooling)，discards long-term non-repetitive dependencies 。最后，将来自关系描述符的池化后的查询/支持表征馈入比较器中。</li>
<li>在训练样本数量有限的情况下定位时间域上有区别的动作块；<br>&emsp;&emsp;聚合具有相等权重的时空块次优。 因此，我们设计了时空注意单元来有区分地聚合。 在低样本状态下，<strong><em>jigsaw and rotation</em></strong>的自监督有助于训练更具鲁棒性的编码器，比较器和注意力。普通的注意(和/或自我监督)不能完全促进本文所述的时间(或空间)排列的不变性</li>
<li>处理此类有区分的长期时域分布偏移（即使对于相同类别的剪辑，这些模式也不会重现在相同的时间位置）</li>
</ol>
<p>贡献：</p>
<ol>
<li>一种鲁棒方法，基于C3D的编码器提取短期依赖特征，生成块表征，随后通过排列不变池化<em>聚合</em>成固定长度表征，形成关系描述符，<em>用于</em>episodic设置[47]中的关系/相似性学习。</li>
<li>在聚类过程中，时空注意力单元重新分配权重。为了改善在低样本训练时的编码器、比较器、注意力单元，本文通过旋转和时空<em>jigsaws</em>拼图引入时空自监督。</li>
<li>一种改进的自监督注意力单元。通过在注意力单元的输入上应用诸如拼图和/或旋转之类的增强模式，并使输出与来自注意力单元的未扩增数据的注意力向量系数的增强方式相同，从而进行对齐。 因此，注意力单元被设计成，对于给定的增强动作是不变的。</li>
<li>提出了新的数据分割方法，以便系统比较few-shot动作识别算法，并适用于现有方法，e.g. 使用不同的pipeline，数据模态形式，数据分割和实验方法的情况下。</li>
</ol>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;<strong>Few-shot action recognition</strong>[17]使用3D的图映射；[35]使用生成模型；[52]使用带分类器的膨胀网络。[58]提出了复合记忆网络，使用键值记忆关联。ProtoGAN[6]提出了一个GAN模型来生成动作原型，以解决少镜头动作识别问题</p>
<h2 id="与现有工作比较"><a href="#与现有工作比较" class="headerlink" title="与现有工作比较"></a>与现有工作比较</h2><p>&emsp;&emsp;<br>与[17]不同，本文使用的是视频剪辑而不是3D骨架坐标。与[52]不同的是，我们使用关系/相似度学习，我们的训练/测试类是不相交的。虽然[58]记忆关键值/帧，但我们对短期和长期依赖性进行建模。虽然[6]通过GAN形成动作原型时，我们专注于自我监督的注意力学习和排列不变聚合。</p>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p><img src="https://i.loli.net/2020/12/22/hmYuwj26PXv71TM.png" alt="quicker_b44bc96c-1035-424e-a484-96bcf3304b24.png"></p>
<h1 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h1><p>为了排除动作识别中常规操作（复杂数据预处理和帧采样步骤）影响，针对每个数据集沿时间模式均匀采样了20帧。<br><img src="https://i.loli.net/2020/12/22/UP2n1gzFqCoXaDO.png" alt="quicker_1a97f88d-75bb-418a-bfc2-699d183c3483.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="1-为公平性，提出了一种benchmark数据分割方法，拆分HMDB51、miniMIT、UCF101数据集的类别，并公开在附录中。相比之前的，增加了验证集。集齐了训练集、（交叉）验证集、测试集。"><a href="#1-为公平性，提出了一种benchmark数据分割方法，拆分HMDB51、miniMIT、UCF101数据集的类别，并公开在附录中。相比之前的，增加了验证集。集齐了训练集、（交叉）验证集、测试集。" class="headerlink" title="1. 为公平性，提出了一种benchmark数据分割方法，拆分HMDB51、miniMIT、UCF101数据集的类别，并公开在附录中。相比之前的，增加了验证集。集齐了训练集、（交叉）验证集、测试集。"></a>1. 为公平性，提出了一种benchmark数据分割方法，拆分HMDB51、miniMIT、UCF101数据集的类别，并公开在附录中。相比之前的，增加了验证集。集齐了训练集、（交叉）验证集、测试集。</h2><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>PROTOGAN [6]<br>Dwivedi S K, Gupta V, Mitra R, Ahmed S, Jain A. IEEE, 2019. ProtoGAN: Towards Few Shot Learning for Action Recognition[C]//2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). , 2019Seoul, Korea (South): : 1308–1316.</li>
<li>GenApp [35]<br>Mishra, A., Verma, V.K., Reddy, M.S.K., Arulkumar, S., Rai, P., Mittal, A.: A generative approach to zero-shot and few-shot action recognition. In: WACV (2018)</li>
<li>CMN [58]<br>Zhu, L., Yang, Y.: Compound memory networks for few-shot video classification. In: ECCV (2018)</li>
</ol>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>行为识别</category>
      </categories>
      <tags>
        <tag>行为识别</tag>
        <tag>FSL</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING&quot;ZSL-action-recognition-survey&quot;</title>
    <url>/2020/11/17/ZSL-action-recognition-survey/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Zero-Shot Action Recognition <span class="keyword">in</span> Videos: A Survey</span><br><span class="line">Valter Luís Estevam Junior a,c,, Helio Pedrini b and David Menotti c</span><br><span class="line">a Federal Institute of Paraná, Irati-PR, 84500-000, Brazil</span><br><span class="line">b Universiy of Campinas, Institute of Computing, Campinas-SP, 13083-852, Brazil</span><br><span class="line">c Federal University of Paraná, Department of Informatics, Curitiba-PR, 81531-970, Brazil</span><br><span class="line">https://arxiv.org/abs/1909.06423</span><br></pre></td></tr></table></figure>
<p>题目：视频中的零次学习行为识别：综述<br>&emsp;2019年预印版v1， 17 Nov 2020更新V2</p>
<p><strong>摘要</strong>：<br>&emsp;&emsp;近年来，零次学习的动作识别已经引起了人们的关注，并且提出了许多方法来识别图像和视频中的物体，事件和动作。 而能够从模型训练过程中不存在的分类中，对实例进行分类的方法存在需求，特别是在自动视频理解的复杂任务中。因为收集，注释和标记视频是困难且费力的任务。我们确定文献中有许多可用的方法，但是，很难对哪些技术可以视为最新技术进行分类。 尽管存在一些有关静态图像和实验方案的零次动作识别的综述，但没有针对视频方面的。 因此，在本文中，我们对包括进行视觉特征提取和语义特征提取的技术在内的方法进行了概述，特别是视频下零次动作识别中的这些特征之间的映射学习。 我们还提供了数据集，实验和方案的完整描述，为计算机视觉研究领域的发展必不可少的未来工作指出了开放性问题和方向。</p>
<a id="more"></a>
<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>&emsp;&emsp;文首提到本文未区分动作识别和行为识别两个概念，一并论之。（butV1版的总结说仅论述了动作识别的ZSL，而行为识别的ZSL还很少。V2改进了吧）</p>
<h1 id="视觉的和语义的标签嵌入"><a href="#视觉的和语义的标签嵌入" class="headerlink" title="视觉的和语义的标签嵌入"></a>视觉的和语义的标签嵌入</h1><h2 id="视觉嵌入"><a href="#视觉嵌入" class="headerlink" title="视觉嵌入"></a>视觉嵌入</h2><p>表1 ZSAR视觉嵌入中的两类方法：人工特征（HF）和深度特征(DF)<br>&emsp;&emsp;但是，由于缺乏运动建模，基于图像的深度模型不适合直接视频表征，如[90]中所述。这个问题可通过考虑时空关系的深度模型来克服；深度模型中，由全连接层来特征。此策略应用于[65]中3D ConvNN[35]；于[100，112，56，29，98]中使用C3D [90]；于[80，70]中使用I3D [12]。<br>&emsp;&emsp;训练3D CNN，要学习比2D CNN更多的参数。 因此，I3D体系结构[12]（图2（c））使用通用的预训练的ImageNet Inception-V1模型[33]作为基础网络，向每个卷积层添加批归一化。为正确探索时空序列 以及（长期依赖关系？），在Inception-V1的最后一个平均池化层之后使用了LSTM层。 另外，还通过引入光流[12]来改善其性能。该模型如图2（c）所示。 I3D模型在Kinetics 数据集[12]上进行训练，从最后一个全连接层中提取视觉表示，从而在[80]和[70]中以256维表示。 由于C3D和I3D模型都在大规模数据集上进行了预训练[56]，这可能违反了ZSL假设（训练和测试集之间的类分离）。 因此，通过使用深度学习技术，出现了一个新的问题。 我们将在第5节中对此进行详细讨论。尽管简单而相对有效，最近的工作（如，对这些现成的全局描述符进行微调，用于对时间或空间关系建模， 或考虑语义信息产生新的特征表征）已显示出显着的性能提高。</p>
<h2 id="语义标签嵌入"><a href="#语义标签嵌入" class="headerlink" title="语义标签嵌入"></a>语义标签嵌入</h2><p>表2 语义嵌入中的方法：标注（A）和单词嵌入（WE）<br>&emsp;&emsp;最流行的语义标签嵌入策略是skip-gram模型[63]（图3e）。它是一种Word2Vec更具体的实现[62]。Word2Vec已有多人采用（参见表2）。 该模型是一种学习向量表示的有效方法，可捕获大量的句法和语义词关系[62]。 该方法包括学习一个基于softmax输出计算单词之间相似度的神经网络。在ZSL中，所需单词（动作标签）的语义矢量表示是基于skip-gram网络的隐藏层中300个神经元的激活 提供此单词作为输入时的跳过语法网络的功能。<br>&emsp;&emsp;执行语义标签嵌入的另一种方法是基于计数的称为全局向量（GloVe）模型[68]。</p>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p>&emsp;&emsp;ZSAR的中心问题是如何使用视觉和语义信息，对未见过的类中的新实例进行分类（即执行传输知识）。 我们确定了三种主要方法：（i）直接分类到语义嵌入空间中，通常在其上投影视觉特征； （ii）分类到中间空间，这个中间空间是由视觉和语义表征的组合生成（如，动作和目标的潜在属性或共现）。（iii）通过考虑语义信息的合成视觉特征，将其分类为视觉嵌入空间，从而为看不见的类生成视觉原型。但是，ZSAR方法结合了多种策略，因此很难提供明确的分类依据。表3给出了根据我们的一般标准进行的方法及其分类，并且还提供了一些观察结果。</p>
<h1 id="评测数据集"><a href="#评测数据集" class="headerlink" title="评测数据集"></a>评测数据集</h1><p>各种数据集介绍</p>
<h1 id="实验与性能分析"><a href="#实验与性能分析" class="headerlink" title="实验与性能分析"></a>实验与性能分析</h1><p>&emsp;&emsp;选用HMBD51, UCF101or Olympic Sports 数据集，因其最多被使用。但没选用ActivityNet，因有修剪前后区别，本文未区分。<br>常用实验方法：</p>
<ol>
<li>数据集的类别随机分为两个不相交的集合，分别称为可见（源）和不可见（目标），比例不同（90％/ 10％，80％/ 20％和50％/ 50％）。 重复此过程多少次（3、5、10、30、50），并且在不做任何事情的情况下，选择的比例和运行次数是合理的。 由于域移位问题，很少有工作采用交叉数据集配置，其中在一个数据集中训练模型，然后在另一个数据集中进行评估。 表5中显示了一个示例，标记为0/20和0/50，与类内方法相比，结果令人印象深刻。 他们的工作通过利用目标对象-动作关系完成迁移学习</li>
<li>三个绩效指标（即总体准确性，每类平均准确性和总平均准确性）。我们将值（四舍五入到小数点后一位），标准差，报告时间和度量类型包括在内</li>
</ol>
<p><strong>GAN的方法</strong>：Zhang and Peng[113]和Mandal等人[59]利用GAN从具有相同统计属性的训练集中生成更多训练数据，并将分类执行到视觉嵌入空间中。此方法具有较高的判别力，并且与其他方法相比，信息退化所带来的影响要小得多。Mandal等人[59]改编了WassersteinGAN，适应于可见和不可见的类别标签的嵌入，并且胜过Zhang和Peng[113]所做的工作。</p>
<p><strong>transductive setting(T) VS inductive setting(I)：</strong>直推式、转导式VS归纳式</p>
<ol>
<li>目前T直推比I归纳式的实验结果好。这得益于自训练和hubness correction中心度校正方法，有效地缓解了域偏移的问题</li>
<li>现实场景中，I归纳式更适合。</li>
</ol>
<p>&emsp;&emsp;另一个重要的是使用<strong>属性方法，通常带来比单词向量更好的性能结果</strong>。然而，属性的方法是不可扩展的，使得在现实情况中，不可实现。so需要更多的研究工作在语义嵌入方面，比如，基于自动属性标注的高级语义描述，目标对象、场景与动作或自然语言描述的关系为基础的高级语义描述。</p>
<h1 id="目前问题和未来研究方向"><a href="#目前问题和未来研究方向" class="headerlink" title="目前问题和未来研究方向"></a>目前问题和未来研究方向</h1><p>&emsp;&emsp;尽管在过去的几年中，零次动作识别已经取得了很大的进步，但它的性能远不及传统的监督学习。ZSL的最佳结果，也就是Mandal等人[59]（38.3±1.0）<strong>使用生成模型</strong>，Gao等人[25]（41.6±1.0）和Mettes and Snoek [61]（40.4±1.0）<strong>使用目标对象及其与动作的关系</strong>，甚至是Jones等人[38]<strong>使用动态属性</strong>。我们可以看到仍然有很多提升空间，这需要解决或改善经典的ZSL问题，即语义鸿沟。<br>&emsp;&emsp;如图4所示，单词向量方法可能会混淆复合词类别（例如，鞍马x骑马）。笔者相信语义嵌入的新策略新方法会被提出。较新的方法有：<br>1、使用同义词  Alexiou et al.[5]<br>2、矢量模型（Sent2Vec）[67]的句子，在[26]中使用。 与使用Word2Vec的结果相比，该模型的速度提高了约1.3倍。<br>3、结合NLP的最新进展</p>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>行为识别</category>
      </categories>
      <tags>
        <tag>行为识别</tag>
        <tag>综述</tag>
        <tag>Zero Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Note &quot;generated_images.clamp_(0., 1.)&quot;</title>
    <url>/2021/03/31/generated_images.clamp_/</url>
    <content><![CDATA[<p>just for pin fuction clamp()</p>
<p>torch.Tensor</p>
<p>clamp_(min, max) → Tensor<br>clamp() 的in-place运算形式</p>
<p>torch.clamp<br>torch.clamp(input, min, max, out=None) → Tensor<br>将输入input张量每个元素的夹紧到区间 [min,max]，并返回结果到一个新张量。</p>
<p>操作定义如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">      | min, <span class="keyword">if</span> x_i &lt; min</span><br><span class="line">y_i = | x_i, <span class="keyword">if</span> min &lt;= x_i &lt;= max</span><br><span class="line">      | max, <span class="keyword">if</span> x_i &gt; max</span><br></pre></td></tr></table></figure>
<hr>
]]></content>
      <categories>
        <category>Note</category>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>fuction</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING&quot;联邦学习与元学习相关的部分&quot;</title>
    <url>/2020/11/01/fed-learning-about-meta-learning/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><p>笔记一些联邦学习中与元学习相关的材料。</p>
<h2 id="1、关于联邦学习的个性化能力综述"><a href="#1、关于联邦学习的个性化能力综述" class="headerlink" title="1、关于联邦学习的个性化能力综述"></a>1、关于联邦学习的个性化能力综述</h2><a id="more"></a>
<p>中文by <a href="https://ereebay.me/posts/54199/" target="_blank" rel="noopener">https://ereebay.me/posts/54199/</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DOI:10.1109/WorldS450073.2020.9210355</span><br><span class="line">Survey of Personalization Techniques <span class="keyword">for</span> Federated Learning</span><br><span class="line">V. Kulkarni, Milind Kulkarni, A. Pant。Published 2020。Vishwakarma University&amp;DeepTek Inc</span><br><span class="line">2020 Fourth World Conference on Smart Trends <span class="keyword">in</span> Systems, Security and Sustainability (WorldS4)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Abstract：Federated learning enables machine learning models to learn from private decentralized data without compromising privacy. The standard formulation of federated learning produces one shared model for all clients. Statistical heterogeneity due to non-IID distribution of data across devices often leads to scenarios where, for some clients, the local models trained solely on their private data perform better than the global shared model thus taking away their incentive to participate in the process. Several techniques have been proposed to personalize global models to work better for individual clients. This paper highlights the need for personalization and surveys recent research on this topic</p>
</blockquote>
<h3 id="3-4Meta-learning"><a href="#3-4Meta-learning" class="headerlink" title="3.4Meta learning"></a>3.4Meta learning</h3><p>&emsp;&emsp;元学习涉及到多个学习任务的训练，以生成能够快速适应的模型，该模型可以通过少量的训练样本就能够快速拟合学习解决新任务。Finn【25】提出了模型无关的元学习算法(MAML)，该算法与使用梯度下降法训练的任何模型都兼容。MAML建立了适用于多个任务的内部表示，因此针对新任务，对于顶层的微调可以产生比较好的结果。</p>
<p>&emsp;&emsp;<strong>Jiang【15】指出可以将联邦学习的过程看做是meta training而personalization过程可以看做是meta testing过程。那么FedAvg【3】算法与Reptile【26】非常相似。同时作者观察到，仔细的微调可以产生准确率高的全局模型，并且比较容易个性化，但是单纯的根据全局模型的准确率来优化模型会损失模型后续的个性化能力。联邦学习的其他个性化方法将全局模型的生成和个性化能力视作两个独立的过程，Jiang【15】提出了一种改进的FedAVG算法，该算法可以同时获得更好的全局模型和更好的个性化模型。</strong></p>
<p>&emsp;&emsp;Fallah【27】在Personalized federated learning: A meta-learning approach,文中提出的标准联邦学习问题的新公式结合了MAML，并试图找出一个全局模型，该模型在每个节点针对其自身的损失函数进行更新后均表现良好。此外，他们提出了Per-FedAvg来解决上述问题。Khodak【28】在Adaptive gradient-based meta-learning methods中提出了ARUBA，并通过将其应用于FedAVG证明了性能的提高。chen【29】在Federated meta-learning for recommendation提出了一个用于构建个性化推荐模型的联邦元学习框架，其中算法和模型都已参数化并且需要优化。</p>
<h3 id="4Discussion"><a href="#4Discussion" class="headerlink" title="4Discussion"></a>4Discussion</h3><p>&emsp;&emsp;<strong>在联邦学习中，当本地节点的数据集很小，且都是IID的情况（是不是和元学习的场景相似）下，全局模型通常会超过本地模型，而且大部分的节点都会受益于联邦学习的过程。然而，当节点拥有充分大量的隐私数据集，并且数据的分布是non-IID的时候，本地模型通常会优于全局模型，而且节点通常不倾向与参与到联邦学习过程中。一个开放的理论问题就是：如何决定什么时候全局模型的表现会优于单节点上的模型。</strong></p>
<p>&emsp;&emsp;这篇文章主要总结了几种用于优化全局模型个性化技术。除了少数的例外，大多数之前的工作都集中在衡量全局模型在聚合的数据上的表现，而不是衡量这些模型在单独节点上的性能。但是如果全局模型会在使用之前进行个性化设置的话，那么全局性能就没有意义。</p>
<p>&emsp;&emsp;个性化模型通常在单节点上的表现能够优于全局模型和本地模型。但是在某些情况下，个性化模型的能力无法达到和本地模型相同的能力，尤其是在差分隐私等情况下。</p>
<h2 id="2、2019年底大佬们综述：联邦学习的进展和开放问题"><a href="#2、2019年底大佬们综述：联邦学习的进展和开放问题" class="headerlink" title="2、2019年底大佬们综述：联邦学习的进展和开放问题"></a>2、2019年底大佬们综述：联邦学习的进展和开放问题</h2><p>中文by <a href="https://xwzheng.gitbook.io/fl" target="_blank" rel="noopener">https://xwzheng.gitbook.io/fl</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Kairouz, Peter, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, 等. </span><br><span class="line">《Advances and Open Problems <span class="keyword">in</span> Federated Learning》. arXiv:1912.04977, 2019年12月10日. </span><br><span class="line">http://arxiv.org/abs/1912.04977.</span><br></pre></td></tr></table></figure></p>
<h3 id="3-3-3-本地微调和元学习"><a href="#3-3-3-本地微调和元学习" class="headerlink" title="3.3.3 本地微调和元学习"></a>3.3.3 本地微调和元学习</h3><p>本地微调，我们指的是通过联邦学习训练单个模型，然后将模型部署到所有的客户端中，并在被用于推断前使用本地的数据集通过额外的训练达到个性化的效果。 这种方法自然地融入了联邦学习模型的通常的生命周期（第1.1.1节）。仍然可以在每轮（例如，100秒）中仅使用少量客户样本进行全球模型的培训；部署模型后，仅发生一次向所有客户端（例如数百万个）广播全局模型。唯一的区别是，在使用模型对客户进行实时预测之前，会进行最终的训练，从而将模型为本地数据集进行个性化。<br>给定一的性能优异的全局模型，对其进行个性化设置的最佳方法是什么？在非联邦学习中，研究人员经常使用微调、迁移学习、域自适应[284,115,56]或者使用本地个性化的模型进行插值。 当然，例如插值等技术，关键在于联邦学习的背景下保证其相应的学习效果。此外，这些技术通常仅假设一对域（源域和目标域），因此可能会丢失联邦学习的一些较丰富的结构。<br>另一种研究个性化和非个性化的方法是通过元学习来进行，这是一种流行的模型适应设定。 在标准的learning-to-learn（LTL）设置中[52]，它对任务上具有一个元分布，用来学习一个学习算法的样本，例如通过发现参数空间的好的约束。 这实际上很好的对应了第3.1节中讨论的统计设定，其中我们对客户端（任务）$i\sim \mathcal{Q}$进行采样，然后从$\mathcal{P_i}$采样该客户端（任务）的数据。<br>最近，已经开发了一种称为模型不可知元学习（MAML）的算法，即元学习全局模型，它可以仅使用几次局部梯度迭代作为学习适合于给定任务的良好模型的起点。 最值得注意的是，流行的Reptile算法[308]的训练阶段与联邦平均[289]密切相关，即Reptile允许服务器的学习率，并且假设所有客户端都拥有相同数量的数据，但其他都是相同的。Khodaketal等人[234]和Jiang等人[217]探索了FL和MAML之间的联系，并展示了MAML的假设是一个可以被联邦学习用于性化模型的相关框架。其他和差分隐私的关系在[260]中被研究。<br>将FL和MAML的思想相结合的总体方向是相对较新的，存在许多未解决的问题：</p>
<pre><code>- 监督任务的MAML算法评估主要集中在合成图像分类问题上[252,331]，其中可以通过对图像类别进行下采样来构造无限的人工任务。用于模拟FL实验的现有数据集建模的FL问题（附录A）可以作为MAML算法的现实基准问题。
- 观察到的全局准确性与个性化准确性之间的差距[217]提出了一个很好的论据，即个性化对于FL至关重要。但是，现有的工作都没有清楚地阐明用于衡量个性化表现的综合指标。例如，对于每个客户来说，小的改进是否比对一部分客户的更大改进更好？相关讨论，请参见第6节。
- Jiang等[217]强调了一个事实，即具有相同结构和性能但经过不同训练的模型可以具有非常不同的个性化能力。尤其是，以最大化全局性能为目标去训模型似乎实际上可能会损害模型的后续个性化能力理解这个问题的根本原因和FL社区与更大的ML社区都相关。
- 在此多任务/LTL框架中，已经开始研究包括个性化和隐私在内的几个具有挑战性的FL命题[234,217,260]。是否还可以通过这种方式分析其他例如概念漂移的问题，比如作为终身学习中的问题[359]？
- 非参数传递LTL算法（例如ProtoNets [363]）是否可以用于FL？
</code></pre><h2 id="3、通过MAML改善联合学习的个性化"><a href="#3、通过MAML改善联合学习的个性化" class="headerlink" title="3、通过MAML改善联合学习的个性化"></a>3、通过MAML改善联合学习的个性化</h2><p>中文by <a href="https://ereebay.me/posts/6350/" target="_blank" rel="noopener">https://ereebay.me/posts/6350/</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Jiang, Yihan, Jakub Konečný, Keith Rush和Sreeram Kannan. </span><br><span class="line">《Improving Federated Learning Personalization via Model Agnostic Meta Learning》. arXiv:1909.12488, 2019年9月27日. </span><br><span class="line">http://arxiv.org/abs/1909.12488.</span><br></pre></td></tr></table></figure><br>Abstract</p>
<ol>
<li>FL算法与MAML具有很多相似性，可以用元学习算法来对其进行解释</li>
<li>微调可以使得gloabl 模型具有更强的准确率，同时更容易做定制化处理</li>
<li>通过标准的中心化数据库训练出来的模型相比Fedavg训练的更难进行定制化处理<br>Introduction</li>
<li>指出了FL与MAML算法的联系，并用MAML算法对FL算法进行解释</li>
<li>对FedAvg进行改进，采用两阶段的训练和fine-tune进行优化</li>
<li>发现FedAvg其实本质是一种metalearning算法，用于优化个性化定制的效果，而不是全局模型的优化。<br>如文献1和2中所提及。</li>
</ol>
<h2 id="4、更快更好的联合学习功能融合方法FedFusion"><a href="#4、更快更好的联合学习功能融合方法FedFusion" class="headerlink" title="4、更快更好的联合学习功能融合方法FedFusion"></a>4、更快更好的联合学习功能融合方法<strong>FedFusion</strong></h2><p>中文by <a href="https://ereebay.me/posts/58531/" target="_blank" rel="noopener">https://ereebay.me/posts/58531/</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DOI:10.1109/ICIP.2019.8803001</span><br><span class="line">Towards Faster and Better Federated Learning: A Feature Fusion Approach</span><br><span class="line">X. Yao, Tianchi Huang, Chenglei Wu, Rui-Xiao Zhang, L. Sun <span class="comment"># 清华</span></span><br><span class="line">Published 2019</span><br><span class="line">2019 IEEE International Conference on Image Processing (ICIP) <span class="comment"># C类会议</span></span><br></pre></td></tr></table></figure><br>Abstract<br>&emsp;&emsp;本文主要提出一种特征融合的方式，来加速并且提升联邦学习的性能。<br>Introduction<br>&emsp;&emsp;如今许多智能设备依赖于预训练模型，这使得机器的推断能力缺乏个性化和灵活性。与此同时，智能终端同时还产生了大量有效的隐私数据，这些数据能够提升这些模型的个性化能力。联邦学习，一种能够直接在终端上对模型进行训练的一种分布式训练算法解决了这个问题。其中以FedAvg算法为代表的的联邦学习算法有效的缓解了在信息交流上的隐私问题，但是后来也有研究表明，联邦学习仍然存在诸如：计算消耗，模型准确率。</p>
<p>&emsp;&emsp;本文提出了一种融合特征的联邦学习算法<strong>FedFusion</strong>，该算法将global模型和local模型的特征进行融合。本文的主要的三个贡献点：</p>
<ol>
<li>引入特征融合机制 </li>
<li>将本地模型和全局模型的特征以一种有效的并且个性化的方式进行融合 </li>
<li>实验表明模型在准确率和泛化能力上都优于baseline并且减少了60%以上的通信量。</li>
</ol>
<h2 id="5、个性化联合学习-一种专注的协作方法FedAMP"><a href="#5、个性化联合学习-一种专注的协作方法FedAMP" class="headerlink" title="5、个性化联合学习:一种专注的协作方法FedAMP"></a>5、个性化联合学习:一种专注的协作方法<strong>FedAMP</strong></h2><p>中文by <a href="https://zhuanlan.zhihu.com/p/260776616" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/260776616</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Huang, Yutao, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei和Yong Zhang. <span class="comment"># 华为</span></span><br><span class="line">《Personalized Federated Learning: An Attentive Collaboration Approach》. arXiv:2007.03797, 2020年7月7日. http://arxiv.org/abs/2007.03797.</span><br><span class="line">V3终版，改为《Personalized Cross-Silo Federated Learning on Non-IID Data》，已由AAAI（CCF A会）接收</span><br></pre></td></tr></table></figure><br>摘要：对于物联网/边缘计算的挑战性计算环境，个性化联合学习允许每个客户端通过以隐私保护的方式与其他客户端进行有效协作来训练强大的个性化云模型。 个性化联合学习的性能在很大程度上取决于客户端之间协作的有效性。 但是，当所有客户端的数据均为非IID时，要在不知道客户端数据分布的情况下推断客户端之间的协作关系就很困难。 在本文中，我们建议通过一个名为联邦关注消息传递（FedAMP）的新颖框架来解决此问题，该框架允许每个客户端在不使用全局模型的情况下共同训练自己的个性化云模型。 FedAMP通过反复鼓励具有更多相似模型参数的客户进行更强的协作，从而实现了一种细心的协作机制。 这可以自适应地发现客户端之间的基础协作关系，从而极大地提高了协作效率，并带来了FedAMP的出色性能。 我们建立了凸模型和非凸模型的FedAMP的收敛性，并进一步提出了一种类似于FedAMP框架的启发式方法，以进一步提高其在深层神经网络的联合学习中的性能。 大量的实验证明了我们的方法在处理非IID数据，脏数据和丢弃的客户端方面的优越性能。</p>
<p>to be continued</p>
<hr>
<h1 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h1>]]></content>
      <categories>
        <category>Paper reading</category>
        <category>其他</category>
      </categories>
      <tags>
        <tag>Fed Learning</tag>
        <tag>Meta Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>如何做好学术报告</title>
    <url>/2020/07/28/how_to_do_Academic_report/</url>
    <content><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>PPT展示，作为一种报告方法，是沟通交流的重要组成，不仅限于学术交流中。在工作中，跟客户展示方案、跟领导述职汇报、项目演示评审，都离不开它。在使用计算机的场景下，想象不到没有它的地方。<br>balabala 很重要。<br>结合刚好看到的文章（见参考），归纳下重点和自己的一点收获。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><a id="more"></a></h2><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>原文开头强调了“提高语言交流能力的重要性”，总结了下句。</p>
<blockquote>
<p>学会高效准确的口头及书面语言表达是一个优秀科学家必须具备的基本能力。<br>这在其他领域，依然是必须具备的基本能力。</p>
</blockquote>
<h1 id="Tips："><a href="#Tips：" class="headerlink" title="Tips："></a>Tips：</h1><h2 id="1、对听众，有针对性的准备。"><a href="#1、对听众，有针对性的准备。" class="headerlink" title="1、对听众，有针对性的准备。"></a>1、对听众，有针对性的准备。</h2><p>听众都是你的细分专业领域的人，那么你可以更展示得专业一些。若不是，那么请科普一点，<strong>通俗易懂也是一种能力</strong>。</p>
<blockquote>
<p>在准备一个学术报告之前，首先要清楚你的听众是谁，有多少听众，他们的专业领域是什么？如果听众的专业背景五花八门，提示你这个报告不能做的太专业，科普一点效果可能会更好。实际上在我们进行很多项目及工作应聘答辩时，大多时候是大同行作为评委。我的体会是，只有知道了你将要讲给谁听，你才能准备好一场有针对性的报告，让你的报告对大多数的听众都有所收获。我以前在美国做教授期间，曾经做过几次新教授招聘委员会的成员（recruiting committee member）。有一次在一位植物学岗位的应聘者答辩完后，与我同系的一位教授推开了我办公室的门，询问我对应聘者学术报告的印象，我说他讲得很好，非常清楚。没想到这位同事却认为该应聘者讲得一塌糊涂，他说他本以为对这位应聘者报告中提到的一些内容是稍微知道一点的，听完他的报告后他反而彻底弄糊涂了。等到全系教授讨论这位应聘者的资格时，他当然没有太多正面的评价。我还有一次经历，那是在2002年的2月初，我申请了UC Davis果树系一个tenure－track assistant professor公开招聘的位置，拿到面试资格后去Davis应聘，我讲完学术报告后，在系大厅的过道里遇到了一位女教授，她非常感谢我的报告，她说她曾经听过与我同领域一位院士所做的四次报告，对里面的一些概念还是一直没弄懂，听完我的报告，她总算完全弄明白怎么回事了。我后来感觉是因为我讲的更科普，连外行都听懂了，反而让这部分人心生感激，他们感觉到没有浪费时间。所以，我经常对我的学生讲，如果你能让台下的听众都听懂了你的学术报告，他们会很感激你，同时也会觉得他们自己很聪明，但他们不知道是你故意让他们有了这种感觉，这就是驾驭听众的能力。</p>
</blockquote>
<h2 id="2、挑重点，以及重中之重。根据允许PPT的时间，可以做到临场调节。但重点一定要说“懂”。"><a href="#2、挑重点，以及重中之重。根据允许PPT的时间，可以做到临场调节。但重点一定要说“懂”。" class="headerlink" title="2、挑重点，以及重中之重。根据允许PPT的时间，可以做到临场调节。但重点一定要说“懂”。"></a>2、挑重点，以及重中之重。根据允许PPT的时间，可以做到临场调节。但重点一定要说<font color="ff0000">“懂”</font>。</h2><blockquote>
<p>报告的内容要根据所给报告时间的长短来定。我们很多的课题答辩报告，通常只给15分钟时间。常常听到很多人抱怨说这么短的时间我能讲什么呢。时间越短说明越要概括，只能讲重点，少讲废话。5分钟有5分钟报告的做法，15分钟有15分钟的讲法，45分钟又有45分钟的讲法。这就好比一篇科学论文的题目，摘要及正文，摘要概括了正文中的要点，而题目又高度综合了文中的内容。</p>
</blockquote>
<h2 id="3、简洁并“准确”的标题，用好关键词。不懂的关键词要自查学好。"><a href="#3、简洁并“准确”的标题，用好关键词。不懂的关键词要自查学好。" class="headerlink" title="3、简洁并“准确”的标题，用好关键词。不懂的关键词要自查学好。"></a>3、简洁并<font color="ff0000">“准确”</font>的标题，用好关键词。不懂的关键词要自查学好。</h2><blockquote>
<p>我认为一场好的报告一定要有好的题目，这个题目一定要越短越好，越高度概括越好，浓缩的才是精华嘛。要让人一看见这个题目就想知道里面的内容，于是吸引很多人来听你的报告。</p>
</blockquote>
<h2 id="4、用好PPT模板，内容简洁，文字不可多。念字是大忌。"><a href="#4、用好PPT模板，内容简洁，文字不可多。念字是大忌。" class="headerlink" title="4、用好PPT模板，内容简洁，文字不可多。念字是大忌。"></a>4、用好PPT模板，内容简洁，文字不可多。念字是大忌。</h2><blockquote>
<p>一个好的报告还要有一套好的ppt，ppt千万不要做的过于花里胡哨。每张ppt的内容不可多，要简洁，讲的内容才放进去，不讲的内容千万不要放。文字不可太多，太多了就会念，用好关键词就可以了。实际上我还是喜欢白底黑字的ppt，这种搭配对眼睛最舒服。</p>
</blockquote>
<h2 id="5、前5张slide，尤为重要。"><a href="#5、前5张slide，尤为重要。" class="headerlink" title="5、前5张slide，尤为重要。"></a>5、前5张slide，尤为重要。</h2><blockquote>
<p>一场45分钟的报告如果最前面的5张ppt没用好，一半人会立马进入梦乡或思想开小差了，效果一定不好。什么样的效果才是好效果呢，要让听众想如果他也能像你一样做报告就好了，你能成为他心中的榜样，表明你的报告的确很精彩。</p>
</blockquote>
<h2 id="6、PPT展示漂亮，得益于充分的准备（反复修改-脱稿-试讲-细节）"><a href="#6、PPT展示漂亮，得益于充分的准备（反复修改-脱稿-试讲-细节）" class="headerlink" title="6、PPT展示漂亮，得益于充分的准备（反复修改+脱稿+试讲+细节）"></a>6、PPT展示漂亮，得益于充分的准备（反复修改+脱稿+试讲+细节）</h2><blockquote>
<p>一段难忘经历。我在全系师生面前做的第一场报告是进行了非常充分的准备的。为了让我好好替她争光，也为了好好培养她的第一个Ph. D.学生，她花费了大量的时间教我如何准备一个seminar。首先，我做的每一张ppt她都会提出修改意见，从文字描述，到每张ppt的布局，颜色搭配等。之后，她还让我写了一份15页的讲稿，进行了反复修改，然后她让我反复念，并纠正我发音不正确的单词，让我讲的时候要有声音的起伏。通过5次试讲，最终完全脱稿演讲。她后来对我说，我讲完后，系里很多教授去祝贺她，说她的学生是留学生中讲得最好的一位，祝贺她有这么好的学生。</p>
<p><strong>万能金句名言：</strong>精心组织、反复修改。充分准备，方能语无伦次，无与伦比的<strong>自信</strong>。</p>
</blockquote>
<hr>
<p>参考：<br>1、<a href="https://mp.weixin.qq.com/s/tTrovn613uvlSSl4JkA6aQ" target="_blank" rel="noopener">黎家教授：如何做好学术报告、写好论文</a></p>
]]></content>
      <categories>
        <category>Meta ability</category>
      </categories>
      <tags>
        <tag>Research</tag>
        <tag>PPT</tag>
        <tag>presentation</tag>
      </tags>
  </entry>
  <entry>
    <title>测试hello！</title>
    <url>/2020/07/19/test1/</url>
    <content><![CDATA[<p>你好！</p>
<h1 id="标题1"><a href="#标题1" class="headerlink" title="标题1"></a>标题1</h1><h2 id="标题1-1"><a href="#标题1-1" class="headerlink" title="标题1.1"></a>标题1.1</h2><h3 id="标题1-1-1"><a href="#标题1-1-1" class="headerlink" title="标题1.1.1"></a>标题1.1.1</h3><a id="more"></a>
<h3 id="标题1-1-2"><a href="#标题1-1-2" class="headerlink" title="标题1.1.2"></a>标题1.1.2</h3><h1 id="标题2"><a href="#标题2" class="headerlink" title="标题2"></a>标题2</h1><h2 id="标题2-1"><a href="#标题2-1" class="headerlink" title="标题2.1"></a>标题2.1</h2><h3 id="标题2-1-1"><a href="#标题2-1-1" class="headerlink" title="标题2.1.1"></a>标题2.1.1</h3><h3 id="标题2-1-2"><a href="#标题2-1-2" class="headerlink" title="标题2.1.2"></a>标题2.1.2</h3><h3 id="标题2-1-3"><a href="#标题2-1-3" class="headerlink" title="标题2.1.3"></a>标题2.1.3</h3><h2 id="标题2-2"><a href="#标题2-2" class="headerlink" title="标题2.2"></a>标题2.2</h2><h3 id="标题2-2-1"><a href="#标题2-2-1" class="headerlink" title="标题2.2.1"></a>标题2.2.1</h3><h3 id="标题2-2-2"><a href="#标题2-2-2" class="headerlink" title="标题2.2.2"></a>标题2.2.2</h3><p>good!</p>
]]></content>
      <categories>
        <category>测试分类</category>
      </categories>
      <tags>
        <tag>ceshi_tags1</tag>
        <tag>tags2</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM预备知识</title>
    <url>/2020/07/26/note-slam-pre/</url>
    <content><![CDATA[<p>视觉slam<br>先说视觉这块，首先射影几何的一些内容相机模型，单视几何，双视几何和多视几何。这些内容可以在<a href="http://www.robots.ox.ac.uk/~vgg/hzbook/这本书中找到。英文版的，另外中科院的吴福朝编著的“计算机视觉中的数学方法”也很好，他涵盖了上述了MVG" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/hzbook/这本书中找到。英文版的，另外中科院的吴福朝编著的“计算机视觉中的数学方法”也很好，他涵盖了上述了MVG</a> in CV book中的大部分内容，强烈安利。</p>
<p>然后是一些视觉特征，这方面就是一些特征，描述子，匹配相关等。见SIFT，ORB、BRISK、SURF等文章。</p>
<p>数学方面首先是三维空间的刚体运动，参考机器人学， 关于优化，SLAM中的优化方法十分基本，参考高斯牛顿，LM，结合稀疏线性代数，其实用的时候会使用一种g2o的图优化库或者ceres。</p>
<p>最难的应该算是李群和李代数，这方面可以参考book state estimation for Robotics。当然不想看书的话可以参考博客<a href="http://www.cnblogs.com/gaoxiang12/tag/%E6%9D%8E%E4%BB%A3%E6%95%B0/。" target="_blank" rel="noopener">http://www.cnblogs.com/gaoxiang12/tag/%E6%9D%8E%E4%BB%A3%E6%95%B0/。</a></p>
<p>为了看论文的时候能够比较流畅，还应该具备一些概率论的知识，这里推荐bookProbabilistic Robotics pdf</p>
<p>话说高翔博士近期完成一本SLAM的入门book，有理论有实践，写的不错，推荐包含了上述在视觉slam需要的所有基础知识，真是造福大众啊。详细研读此书，以后读各种论文就不会显得那么吃力了吧。最后列举一些玩slam的一些必备工具和相关资源。</p>
<p>tools<br>ubuntu, install, cmake, bash, vim, qt(optional).<br>OpenCV install, read the opencv reference manual and tutorial<br>ros, install, [tutorial}(<a href="http://wiki.ros.org/ROS/Tutorials" target="_blank" rel="noopener">http://wiki.ros.org/ROS/Tutorials</a>).<br>python. 可以使用pycharm,作为IDE.<br>为什么使用ubuntu？因为大家的代码，全是用linux，而且很多使用ros的，ros一定是要Linux的，同时还要cmake。Ubuntu是比较适合初学Linux的人，非常好用</p>
<p>somethind about Calibration<br>opencv camera Calibration<br>matlab camera Calibration toolbox<br>svo camera Calibration<br>ros wiki camera Calibration<br>为什么要标定相机呢，因为slam的模型中假设 相机的内参数是已知的，因此有了这个内参数我们才能正确的初始化slam系统。</p>
<p>ROS<br>ros usually used pakcage<br>svo<br>orb slam<br>ar_tracker_alvar githun page ros page<br>ros ptam,原始代码不支持ros, 这里给出ros版本的代码. 原始代码网站<br>DSO<br>ros books<br>Learning ROS for Robotics Programming<br>机器人操作系统（ROS）浅析<br>some blogs about ros<br><a href="http://www.guyuehome.com/page/1" target="_blank" rel="noopener">http://www.guyuehome.com/page/1</a><br>SLAM基础学习<br>Multiple View Geometry in Computer Vision。这本书基本涵盖了Vision-based SLAM这个领域的全部理论基础！读多少遍都不算多！另外建议配合Berkeley的课件学习。（更新：这本书书后附录也可以一并读完，包括附带bundle adjustment最基本的levenberg marquardt方法，newton方法等）．<br>Sparse Matrix，这是大型稀疏矩阵处理的一般办法。可以参考Dr. Tim Davis的课件：Tim Davis ，他的主页里有全部的课程视频和Project。针对SLAM问题，最常用的least square算法是Sparse Levenberg Marquardt algorithm，这里有一份开源的代码以及具体实现的paper：Sparse Non-Linear Least Squares in C/C++<br>openSLAM<br>dataset tum<br>PCL<br>opencv<br>推荐阅读的书<br>Multiple View Geometry in Computer Vision<br>Probabilistic Robotics pdf<br>state estimation for Robotics<br>Quaternion kinematics for the error-state KF<br>凸优化，<a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener">https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf</a><br>线性系统理论，<a href="https://www.amazon.com/Linear-System-Electrical-Computer-Engineering/dp/0199959579" target="_blank" rel="noopener">https://www.amazon.com/Linear-System-Electrical-Computer-Engineering/dp/0199959579</a><br>An Invitation to 3-D Vision，<a href="https://www.eecis.udel.edu/~cer/arv/readings/old_mkss.pdf" target="_blank" rel="noopener">https://www.eecis.udel.edu/~cer/arv/readings/old_mkss.pdf</a><br>Modern Control Systems，<a href="https://www.amazon.com/Modern-Control-Systems-12th-Richard/dp/0136024580" target="_blank" rel="noopener">https://www.amazon.com/Modern-Control-Systems-12th-Richard/dp/0136024580</a><br>Rigid Body Dynamics，<a href="http://authors.library.caltech.edu/25023/1/Housner-HudsonDyn80.pdf。说实话刚体动力学理论我没有找到特别好的书。但是刚体动力学理论很重要。" target="_blank" rel="noopener">http://authors.library.caltech.edu/25023/1/Housner-HudsonDyn80.pdf。说实话刚体动力学理论我没有找到特别好的书。但是刚体动力学理论很重要。</a><br>Feedback Systems: An Introduction for Scientists and Engineers，FBSwiki<br>《机器学习》，周志华老师的书。<br>线性估计，<a href="https://www.amazon.com/Linear-Estimation-Thomas-Kailath/dp/0130224642" target="_blank" rel="noopener">https://www.amazon.com/Linear-Estimation-Thomas-Kailath/dp/0130224642</a><br>vision Navigation<br>Georg Klein and David Murray, “Parallel Tracking and Mapping for Small AR Workspaces”, In Proc. International Symposium on Mixed and Augmented Reality (ISMAR’07, Nara).<br>D. Scaramuzza, F. Fraundorfer, “Visual Odometry: Part I - The First 30 Years and Fundamentals IEEE Robotics and Automation Magazine”, Volume 18, issue 4, 2011.<br>F. Fraundorfer and D. Scaramuzza, “Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications,” in IEEE Robotics &amp; Automation Magazine, vol. 19, no. 2, pp. 78-90, June 2012. doi: 10.1109/MRA.2012.2182810<br>A Kalman Filter-Based Algorithm for IMU-Camera Calibration Observability Analysis and Performance Evaluation<br>SVO- Fast Semi-Direct Monocular Visual Odometry<br>eth zasl sensor,<a href="http://wiki.ros.org/ethzasl_sensor_fusion" target="_blank" rel="noopener">http://wiki.ros.org/ethzasl_sensor_fusion</a><br>Stephan Weiss. Vision Based Navigation for Micro Helicopters PhD Thesis, 2012 pdf<br>Stephan Weiss, Markus W. Achtelik, Margarita Chli and Roland Siegwart. Versatile Distributed Pose Estimation and Sensor Self-Calibration for Autonomous MAVs. in IEEE International Conference on Robotics and Automation (ICRA), 2012. pdf<br>Stephan Weiss, Davide Scaramuzza and Roland Siegwart, Monocular-SLAM–based navigation for autonomous micro helicopters in GPS-denied environments, Journal of Field Robotics (JFR), Vol. 28, No. 6, 2011, 854-874. pdf<br>Stephan Weiss and Roland Siegwart. Real-Time Metric State Estimation for Modular Vision-Inertial Systems. in IEEE International Conference on Robotics and Automation (ICRA), 2011. pdf<br>Simon Lynen, Markus Achtelik, Stephan Weiss, Margarita Chli and Roland Siegwart, A Robust and Modular Multi-Sensor Fusion Approach Applied to MAV Navigation. in Proc. of the IEEE/RSJ Conference on - - Intelligent Robots and Systems (IROS), 2013. pdf<br>[orb slam]<br>Raúl Mur-Artal, J. M. M. Montiel and Juan D. Tardós. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015. (2015 IEEE Transactions on Robotics Best Paper Award). PDF.<br>Dorian Gálvez-López and Juan D. Tardós. Bags of Binary Words for Fast Place Recognition in Image Sequences. IEEE Transactions on Robotics, vol. 28, no. 5, pp. 1188-1197, 2012.</p>
<p>参考<br>大疆的YY硕<br><a href="https://www.zhihu.com/question/24492974/answer/29987148" target="_blank" rel="noopener">https://www.zhihu.com/question/24492974/answer/29987148</a><br><a href="https://zhuanlan.zhihu.com/p/22266788" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22266788</a><br>转载：<a href="https://x007dwd.github.io/2017/03/02/SLAM-prerequisite/" target="_blank" rel="noopener">https://x007dwd.github.io/2017/03/02/SLAM-prerequisite/</a> Posted by Bobin on March 2, 2017</p>
]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>Useful Links for Scholar</title>
    <url>/2022/03/21/useful-links-SC/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><p>&emsp;&emsp;记录一些有用的Link。</p>
<h2 id="查文献"><a href="#查文献" class="headerlink" title="查文献"></a>查文献</h2><h3 id="常用的"><a href="#常用的" class="headerlink" title="常用的"></a>常用的</h3><ol>
<li><a href="https://www.semanticscholar.org/" target="_blank" rel="noopener">Semantic Scholar</a><br>常用1</li>
<li><a href="https://www.connectedpapers.com/" target="_blank" rel="noopener">Connected Papers</a><br>图形化地查一篇文章的引用与被引。</li>
<li><p><a href="https://scholar.google.com/" target="_blank" rel="noopener">Google Scholar</a><br>常用2。打不开？！见后面5个镜像站：</p>
<ul>
<li><a href="https://ac.scmor.com/" target="_blank" rel="noopener">谷歌学术镜像和其他资源</a></li>
<li><a href="https://scholar.chongbuluo.com/" target="_blank" rel="noopener">学术搜索 - 虫部落</a></li>
<li><a href="https://sc.panda321.com/" target="_blank" rel="noopener">熊猫学术-Google scholar</a></li>
<li><a href="https://www.library.ac.cn/" target="_blank" rel="noopener">Mirror List</a></li>
<li><a href="https://gfsoso.99lb.net/" target="_blank" rel="noopener">谷粉学术</a></li>
</ul>
</li>
<li><p><a href="https://arxiv.org/" target="_blank" rel="noopener">arXiv官网</a><br>常用3。没发表和即将发表的论文提前看</p>
</li>
<li><a href="http://xxx.itp.ac.cn/" target="_blank" rel="noopener">cn.arxiv</a><br>arXiv国内镜像站</li>
<li><a href="http://www.arxiv-sanity.com/" target="_blank" rel="noopener"><del>Arxiv更友好的展示 | Sanity</del></a><br>疑似停站</li>
<li><a href="https://arxiv-sanity-lite.com/" target="_blank" rel="noopener">arxiv-sanity Lite版</a><br>换种视角来逛arXiv，支持订阅</li>
<li><a href="https://paperswithcode.com/" target="_blank" rel="noopener">Papers With Code</a><br>Paper连接Code，（数据集）榜单，Task。</li>
</ol>
<a id="more"></a>
<h3 id="会议集官网"><a href="#会议集官网" class="headerlink" title="会议集官网"></a>会议集官网</h3><ol>
<li><a href="https://openaccess.thecvf.com/menu" target="_blank" rel="noopener">CVPR+ICCV+WACV</a></li>
<li><a href="https://www.ecva.net/index.php" target="_blank" rel="noopener">ECCV的总集 | ECVA</a></li>
<li><a href="https://www.aaai.org/Library/AAAI/aaai-library.php" target="_blank" rel="noopener">AAAI官网</a></li>
<li><a href="https://proceedings.neurips.cc/" target="_blank" rel="noopener">NeurIPS官方论文集</a></li>
<li><a href="https://openreview.net/" target="_blank" rel="noopener">OpenReview</a><br>部分会议在此公开Review全过程</li>
</ol>
<h3 id="专利"><a href="#专利" class="headerlink" title="专利"></a>专利</h3><ol>
<li><a href="http://www.soopat.com/" target="_blank" rel="noopener">SooPAT 专利搜索（优选2）</a></li>
<li><a href="http://www.innojoy.com/search/home.html" target="_blank" rel="noopener">专利查询（优选1）</a></li>
<li><a href="https://www.drugfuture.com/cnpat/cn_patent.asp" target="_blank" rel="noopener"><del>中国专利下载</del></a></li>
</ol>
<h3 id="会议排名"><a href="#会议排名" class="headerlink" title="会议排名"></a>会议排名</h3><ol>
<li><a href="https://www.guide2research.com/topconf/" target="_blank" rel="noopener">Top Conferences -Ranking</a><br>计算机科学相关会议排名。自开发一个油猴插件（<a href="https://wx.mail.qq.com/ftn/download?func=3&amp;key=c8994c658b45ef2cfaea1765633862380c633965613862381249085d525e005a5254145054095b1503570e564c015b0d0e4b0b00555c040f53020956035a463874095703416b07594505513a15590f485214540a0f53074168155a170848164b191c5015758254e576fb68fda223815d862523edea92f946cf&amp;code=7f9ea8b8&amp;k=c8994c658b45ef2cfaea1765633862380c633965613862381249085d525e005a5254145054095b1503570e564c015b0d0e4b0b00555c040f53020956035a463874095703416b07594505513a15590f485214540a0f53074168155a170848164b191c5015758254e576fb68fda223815d862523edea92f946cf&amp;fweb=1&amp;cl=1" target="_blank" rel="noopener">Conf Search_tampermonkey_scripts.zip</a>），在这个页面增加搜索功能。</li>
<li><a href="http://www.conferenceranks.com/" target="_blank" rel="noopener">Conference Ranks</a></li>
</ol>
<h3 id="会议截稿时间"><a href="#会议截稿时间" class="headerlink" title="会议截稿时间"></a>会议截稿时间</h3><ol>
<li><a href="https://jackietseng.github.io/conference_call_for_paper/conferences.html" target="_blank" rel="noopener">conference_call_for_paper</a></li>
<li><a href="https://jackietseng.github.io/conference_call_for_paper/conferences-with-ccf.html" target="_blank" rel="noopener">conferences-with-ccf [call for paper]</a><br>这两个原作者疑似弃坑。虽很久未更新DDL数据，但也可参考。</li>
<li><a href="https://www.call4paper.com/search?type=event&amp;t=&amp;category=&amp;query=computer+vision" target="_blank" rel="noopener">Call For Papers - The World’s Largest Index/List of Call For Papers</a></li>
</ol>
<h3 id="中科院分区表"><a href="#中科院分区表" class="headerlink" title="中科院分区表"></a>中科院分区表</h3><ol>
<li><a href="https://www.yuque.com/scientometrics/fenqubiao_blog" target="_blank" rel="noopener">期刊分区表博客 · 语雀</a></li>
<li><a href="https://earlywarning.fenqubiao.com/" target="_blank" rel="noopener">国际期刊预警名单</a></li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol>
<li><a href="https://dblp.uni-trier.de/" target="_blank" rel="noopener">[查文献bib] dblp官网</a><br>bib信息不全？搜下试试</li>
<li><a href="https://www.zhihu.com/column/zotero" target="_blank" rel="noopener">Zotero文献生态 - 知乎</a><br>Zotero文献管理软件，真香，习惯了比Endnote香</li>
<li><a href="https://editor.citationstyles.org/about/" target="_blank" rel="noopener">引文样式csl查找</a><br>Zotero csl样式，用于在纯文本/word。</li>
<li><a href="https://distill.pub/" target="_blank" rel="noopener">Distill — Latest articles about machine learning</a></li>
<li><a href="https://www.paperdigest.org/conference-paper-digest/" target="_blank" rel="noopener">文献一句话概述Conference Paper Digest – Paper Digest</a></li>
<li><a href="https://www.graviti.cn/open-datasets" target="_blank" rel="noopener">公开数据集收集 | Graviti 格物钛</a></li>
<li><a href="https://ijournal.topeditsci.com/search?keywordType=title&amp;keyword=&amp;ifStart2019=&amp;ifEnd2019=&amp;jcr=&amp;sub=&amp;isDomestic=&amp;selfCitingRate=all&amp;compatriotRate=all&amp;totalReviewRatio=all&amp;categoryId=1aa5cec81cf6470ffe5a5c7cad245756&amp;pageNum=1&amp;order=impactFactor_2019&amp;orderType=desc" target="_blank" rel="noopener">期刊查询与选择</a></li>
<li><a href="https://www.xueky.com/xuankan.html" target="_blank" rel="noopener">查期刊影响因子+智能选刊助手-Scidown</a></li>
<li><a href="https://www.scimagojr.com/index.php" target="_blank" rel="noopener">查期刊 优选</a></li>
<li><a href="https://readpaper.com/" target="_blank" rel="noopener">readpaper-粤港澳大湾区</a></li>
<li><a href="https://www.aminer.cn/" target="_blank" rel="noopener">AMiner - 找人</a></li>
</ol>
<h2 id="如何阅读文章"><a href="#如何阅读文章" class="headerlink" title="如何阅读文章"></a>如何阅读文章</h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/280750898" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/280750898</a>  吴恩达-如何有效阅读论文<br>四问<ol>
<li>作者想要完成什么（已经完成了什么）？</li>
<li>新方法的关键元素是什么？</li>
<li>对我有用吗？</li>
<li>想看他的哪些参考文献？</li>
</ol>
</li>
</ol>
<h2 id="团队或学者"><a href="#团队或学者" class="headerlink" title="团队或学者"></a>团队或学者</h2><p>待整理，以下简单搬运自 <a href="https://sites.google.com/view/honggangchen/" target="_blank" rel="noopener">陈洪刚教授的主页</a></p>
<ul>
<li><p>Lab/Group<br> Multimedai Lab (CUHK)</p>
<p> Video Processing Lab (UCSD)</p>
<p> Vision and Learning Lab (UCM)</p>
<p> NYU Video Lab (NYU)</p>
<p> Machine Learning Group (Adelaide)</p>
<p> Laboratory for Image &amp; Video Engineering (TEXAS)</p>
<p> Image and Video Processing Lab (NU)</p>
<p> Computer Vision and Image Processing (UoD)</p>
<p> Spatial and Temporal Restoration, Understanding and Compression Team (PKU)</p>
<p> Computational Imaging Group (TUT)</p>
<p> Computer Vision Laboratory (SNU)</p>
<p> Bio Imaging &amp; Signal Processing Lab</p>
<p> DUT MEDIA LAB</p>
</li>
<li><p>Scholars’ Homepage</p>
<p>  B</p>
<pre><code>Alan Bovik
</code></pre><p>  C</p>
<pre><code>Mingming Chen   Chen Chen     Wan-Chi Siu       Yunjin Chen
</code></pre><p>  D</p>
<pre><code>Weisheng Dong
</code></pre><p>  E</p>
<pre><code>Michael Elad
</code></pre><p>  F</p>
<pre><code>Alessandro Foi      Sina Farsiu     Xueyang Fu
</code></pre><p>  G</p>
<pre><code>Xinbo Gao       Zongming Guo      Shuhang Gu
</code></pre><p>  H</p>
<pre><code>Kaiming He      Jia-bin Huang     Jun-jie Huang     Zhe Hu      Muhammad Haris
</code></pre><p>  I</p>
<pre><code>Michal Irani
</code></pre><p>  J</p>
<pre><code>Jiaya Jia      Junjun Jiang 
</code></pre><p>  L</p>
<pre><code>Weisi Lin    Ce Liu     Baoxin Li       Chen Change (Cavan) Loy     Enming Luo      Xin Liu    

Renjie Liao     Xianming Liu      Sifei Liu     Zhouchen Lin      Liang Lin     Leida Li      Jiaying Liu    

Weisheng Lai      Dong Liu      Stamatios Lefkimmiatis
</code></pre><p>  M</p>
<pre><code>Peyman Milanfar     Kai-Kuang Ma      Kede Ma
</code></pre><p>  N</p>
<pre><code>Truong Q. Nguyen
</code></pre><p>  P</p>
<pre><code>Jinshan Pan
</code></pre><p>  R</p>
<pre><code>Wenqi Ren    
</code></pre><p>  S</p>
<pre><code>Chunhua Shen     Jian Sun     Eero Simoncelli     Xiaoyong Shen
</code></pre><p>  T</p>
<pre><code>Radu Timofte     Yapeng Tian     Ying Tai     Dacheng Tao     Robby T. Tan
</code></pre><p>  W</p>
<pre><code>Xiaolin Wu      Yao Wang     Xiaogang Wang     Linfeng Wang     Bihan Wen     Keze Wang    

Xiushen Wei     Zhou Wang     Zhangyang Wang
</code></pre><p>  X</p>
<pre><code>Li Xu     Xiangyu Xu     Ruiqing Xiong     Jiu Xu
</code></pre><p>  Y</p>
<pre><code>Ming-Hsuan Yang    Jianchao Yang     Huanjing Yue    Hantao Yao
</code></pre><p>  Z</p>
<pre><code>Lei Zhang     Zhihua Zhou      Wangmeng Zuo     Yongbing Zhang     Xinfeng Zhang   

Jian Zhang     Yulun Zhang     Yang Zhao     He Zhang    Liming Zhao     Zhiyuan Zha   
</code></pre></li>
<li><p>Scholars’ Github</p>
<p> Bihan Wen (State-of-the-art denoising)</p>
<p> Zehao Huang (Super-resolution benchmark)</p>
<p> Wenhan Yang</p>
<p> Kai Zhang</p>
</li>
</ul>
<hr>
<p>REF</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/22323250" target="_blank" rel="noopener">ResearchGo:研究生活第一帖——文献检索与管理</a></li>
<li><a href="https://sites.google.com/view/honggangchen/" target="_blank" rel="noopener">陈洪刚教授的主页</a></li>
</ol>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Links</tag>
      </tags>
  </entry>
  <entry>
    <title>笔记 ROS2 通信架构</title>
    <url>/2020/07/26/ros2-note/</url>
    <content><![CDATA[<p>计算图级</p>
<h1 id="1-master和nodel"><a href="#1-master和nodel" class="headerlink" title="1 master和nodel"></a>1 master和nodel</h1><h2 id="1-1-master的作用："><a href="#1-1-master的作用：" class="headerlink" title="1.1 master的作用："></a>1.1 master的作用：</h2><p>1、每个node启动时，都需向master注册<br>2、管理node之间的通信<br>启动命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ roscore</span><br></pre></td></tr></table></figure><br><a id="more"></a><br>  <img src="https://pics.images.ac.cn/image/5f1d6f070b52a.html" alt=""></p>
<h2 id="1-2-node"><a href="#1-2-node" class="headerlink" title="1.2 node"></a>1.2 node</h2><p>1、一个进程(process)<br>2、可执行文件（通常为C++编译生成的可执行文件、Python脚本）被执行的实例<br>启动node命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rosrun pkg_name node_name</span><br></pre></td></tr></table></figure><br>rosnode命令：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rosnode命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rosnode list</td>
<td style="text-align:center">@列出当前运行的node信息</td>
</tr>
<tr>
<td style="text-align:left">rosnode info node_name</td>
<td style="text-align:center">@显示出node的详细信息</td>
</tr>
<tr>
<td style="text-align:left">rosnode kill node_name</td>
<td style="text-align:center">@结束某个node</td>
</tr>
<tr>
<td style="text-align:left">rosnode ping</td>
<td style="text-align:center">测试连接节点</td>
</tr>
<tr>
<td style="text-align:left">rosnode machine</td>
<td style="text-align:center">列出在特定机器或列表机器上运行的节点</td>
</tr>
<tr>
<td style="text-align:left">rosnode cleanup</td>
<td style="text-align:center">清除不可到达节点的注册信息</td>
</tr>
</tbody>
</table>
</div>
<h1 id="2-launch文件"><a href="#2-launch文件" class="headerlink" title="2 launch文件"></a>2 launch文件</h1><p>（集成）启动master和多个node：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ roslaunch [pkg_name] [file_name.launch]</span><br></pre></td></tr></table></figure><br>launch文件写法，同遵循xml格式规范。如下.<br>  <img src="https://pics.images.ac.cn/image/5f1d6f086b5e4.html" alt=""></p>
<h1 id="3-通信方式"><a href="#3-通信方式" class="headerlink" title="3 通信方式"></a>3 通信方式</h1><p>以下四种：</p>
<h2 id="3-1-Topic-主题"><a href="#3-1-Topic-主题" class="headerlink" title="3.1 Topic 主题"></a>3.1 Topic 主题</h2><p>特点：异步通信、 publish/ subscriber<br>数据类型：定义在<code>*.msg</code>中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rostopic命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rostopic list</td>
<td style="text-align:center">@列出当前所有的topic</td>
</tr>
<tr>
<td style="text-align:left">rostopic info /topic_name</td>
<td style="text-align:center">@显示某个topic的属性信息</td>
</tr>
<tr>
<td style="text-align:left">rostopic echo /topic_name</td>
<td style="text-align:center">@显示某个topic的内容</td>
</tr>
<tr>
<td style="text-align:left">rostopic pub /topic_name …</td>
<td style="text-align:center">@向某个topic发布内容</td>
</tr>
<tr>
<td style="text-align:left">rostopic bw /topic_name</td>
<td style="text-align:center">查看某个topic的带宽</td>
</tr>
<tr>
<td style="text-align:left">rostopic hz /topic_name</td>
<td style="text-align:center">查看某个topic的频率</td>
</tr>
<tr>
<td style="text-align:left">rostopic find /topic_type</td>
<td style="text-align:center">查找某个类型的topic</td>
</tr>
<tr>
<td style="text-align:left">rostopic type /topic_name</td>
<td style="text-align:center">查看某个topic的类型(msg)</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rosmsg命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rosmsg list</td>
<td style="text-align:center">列出系统上所有的msg</td>
</tr>
<tr>
<td style="text-align:left">rosmsg show /msg_name</td>
<td style="text-align:center">显示某个msg的内容</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-2-Service-服务"><a href="#3-2-Service-服务" class="headerlink" title="3.2 Service 服务"></a>3.2 Service 服务</h2><p>特点：同步通信、reques-reply方式<br>数据类型：定义在<code>*.srv</code>中</p>
<h3 id="topic-VS-service"><a href="#topic-VS-service" class="headerlink" title="topic VS service"></a>topic VS service</h3><p>我们对比一下这两种最常用的通信方式，加深我们对两者的理解和认识，具体见下表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">Topic</th>
<th style="text-align:center">Service</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">通信方式</td>
<td style="text-align:center">异步通信</td>
<td style="text-align:center">同步通信</td>
</tr>
<tr>
<td style="text-align:center">实现原理</td>
<td style="text-align:center">TCP/IP</td>
<td style="text-align:center">TCP/IP</td>
</tr>
<tr>
<td style="text-align:center">通信模型</td>
<td style="text-align:center">Publish-Subscribe</td>
<td style="text-align:center">Request-Reply</td>
</tr>
<tr>
<td style="text-align:center">映射关系</td>
<td style="text-align:center">Publish-Subscribe(多对多)</td>
<td style="text-align:center">Request-Reply（多对一）</td>
</tr>
<tr>
<td style="text-align:center">特点</td>
<td style="text-align:center">接受者收到数据会回调（Callback）</td>
<td style="text-align:center">远程过程调用（RPC）服务器端的服务</td>
</tr>
<tr>
<td style="text-align:center">应用场景</td>
<td style="text-align:center">连续、高频的数据发布</td>
<td style="text-align:center">偶尔使用的功能/具体的任务</td>
</tr>
<tr>
<td style="text-align:center">举例</td>
<td style="text-align:center">激光雷达、里程计发布数据</td>
<td style="text-align:center">开关传感器、拍照、逆解计算</td>
</tr>
</tbody>
</table>
</div>
<p>注意：远程过程调用<code>(Remote Procedure Call，RPC)</code>,可以简单通俗的理解为在一个进程里调用另一个进程的函数。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rosservice 命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rosservice list</td>
<td style="text-align:center">@显示服务列表</td>
</tr>
<tr>
<td style="text-align:left">rosservice info</td>
<td style="text-align:center">@打印服务信息</td>
</tr>
<tr>
<td style="text-align:left">rosservice type</td>
<td style="text-align:center">打印服务类型</td>
</tr>
<tr>
<td style="text-align:left">rosservice uri</td>
<td style="text-align:center">打印服务ROSRPC uri</td>
</tr>
<tr>
<td style="text-align:left">rosservice find</td>
<td style="text-align:center">按服务类型查找服务</td>
</tr>
<tr>
<td style="text-align:left">rosservice call service_name args</td>
<td style="text-align:center">@使用所提供的args调用服务</td>
</tr>
<tr>
<td style="text-align:left">rosservice args</td>
<td style="text-align:center">打印服务参数</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rossrv 命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rossrv show</td>
<td style="text-align:center">@显示服务描述</td>
</tr>
<tr>
<td style="text-align:left">rossrv list</td>
<td style="text-align:center">@列出所有服务</td>
</tr>
<tr>
<td style="text-align:left">rossrv md5</td>
<td style="text-align:center">显示服务md5sum</td>
</tr>
<tr>
<td style="text-align:left">rossrv package</td>
<td style="text-align:center">列出包中的服务</td>
</tr>
<tr>
<td style="text-align:left">rossrv packages</td>
<td style="text-align:center">列出包含服务的包</td>
</tr>
</tbody>
</table>
</div>
<p><strong>tips</strong>： 无论我们定义了srv，或msg，修改 package.xml和CMakeList.txt<strong><em>添加依赖</em></strong>都是必不可少的一步。</p>
<h2 id="3-3-Parameter-Service-参数服务器"><a href="#3-3-Parameter-Service-参数服务器" class="headerlink" title="3.3 Parameter Service 参数服务器"></a>3.3 Parameter Service 参数服务器</h2><p>存储各种参数的字典，可用命令行、<code>launch</code>文件和<code>node（api）</code>读写。<br>1、 命令行</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">rosparam 命令</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">rosparam set param_key param_value</td>
<td style="text-align:center">设置参数</td>
</tr>
<tr>
<td style="text-align:left">rosparam get param_key</td>
<td style="text-align:center">显示参数</td>
</tr>
<tr>
<td style="text-align:left">rosparam load file_name</td>
<td style="text-align:center">从文件（yaml）加载参数</td>
</tr>
<tr>
<td style="text-align:left">rosparam dump file_name</td>
<td style="text-align:center">保存参数到文件（yaml）</td>
</tr>
<tr>
<td style="text-align:left">rosparam delete param_key</td>
<td style="text-align:center">删除参数</td>
</tr>
<tr>
<td style="text-align:left">rosparam list</td>
<td style="text-align:center">列出参数名称</td>
</tr>
</tbody>
</table>
</div>
<p>2、 launch文件<br>与参数服务器相关的标签只有两个，一个是<code>&lt;param&gt;</code>，另一个是<code>&lt;rosparam&gt;</code><br>3、通过node<strong>（API）</strong>设置</p>
<h2 id="3-4-Actionlib-动作库"><a href="#3-4-Actionlib-动作库" class="headerlink" title="3.4 Actionlib 动作库"></a>3.4 Actionlib 动作库</h2><p>类似于Service，带有状态反馈的通信方式。通常用在长时间、可抢占的任务中。<br><img src="https://pics.images.ac.cn/image/5f1d6f07c1032.html" alt=""></p>
<p>客户端会向服务器发送目标指令和取消动作指令,而服务器则可以给客户端发送实时的状态信息,结果信息,反馈信息等等。<br>定义在<code>*.action</code>中。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>ROS</tag>
        <tag>note</tag>
      </tags>
  </entry>
  <entry>
    <title>XMind安卓版和PC端同步BY第三方云端</title>
    <url>/2020/08/13/xmind-Android-sync-by-onedrive/</url>
    <content><![CDATA[<h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><p>之前一直使用幕布，由于种种原因（私档泄露、免费版功能已不能满足需求），不想使用此软件了。<br>故转而找替代产品。<br>定位需求是：<br>1、free；<br>2、思维导图基本功能（大纲），能图片更好；<br>3、多端多台（windows+Android）同步。</p>
<p>此时，XMind出现了。虽然同步上有点小问题，但能完美解决。就是它。<br><a id="more"></a></p>
<h2 id="XMind同步问题"><a href="#XMind同步问题" class="headerlink" title="XMind同步问题"></a>XMind同步问题</h2><p><a href="https://support.xmind.net/hc/zh-cn/articles/360028527611" target="_blank" rel="noopener">XMind安卓版和PC端同步</a>不“完美”，IOS版官方有完美同步教程。</p>
<hr>
<h1 id="同步方案"><a href="#同步方案" class="headerlink" title="同步方案"></a>同步方案</h1><p>借助第三方云（本人使用OneDirve、类似云有许多，坚果云也可以）：<br>1、PC端同步至云（文件保存在OneDrive目录下);<br>2、Android端同步至云（本人使用FolderSync（名气大啊），类似软件有Autosync for OneDrive等）</p>
<h2 id="PC端同步至OneDrive"><a href="#PC端同步至OneDrive" class="headerlink" title="PC端同步至OneDrive"></a>PC端同步至OneDrive</h2><p>这步简单。</p>
<h2 id="使用FolderSync，Android端同步至OneDrive"><a href="#使用FolderSync，Android端同步至OneDrive" class="headerlink" title="使用FolderSync，Android端同步至OneDrive"></a>使用FolderSync，Android端同步至OneDrive</h2><p>FolderSync这类软件获取，需借梯子。安装后，继续下步。</p>
<blockquote>
<p>借鉴<a href="http://help.jianguoyun.com/?p=2887" target="_blank" rel="noopener">如何使用FolderSync在安卓手机上同步文件夹到坚果云？</a></p>
<blockquote>
<p>FolderSync 是一款Android 端的文件同步工具，可以将手机中的文件自动同步到云端空间或者PC端。它目前支援 OneDrive, Dropbox, SugarSync, Box.net,  LiveDrive,  HiDrive,  Google Docs,  NetDocuments,  Amazon S3、 FTP,  FTP,  SFTP，WebDAV 。</p>
<h3 id="添加账户OneDrive"><a href="#添加账户OneDrive" class="headerlink" title="添加账户OneDrive"></a>添加账户OneDrive</h3><p>注意此步不能添加OneDrive校园账户</p>
</blockquote>
</blockquote>
<h3 id="添加同步的文件夹"><a href="#添加同步的文件夹" class="headerlink" title="添加同步的文件夹"></a>添加同步的文件夹</h3><p>名称：随便起（e.g. XMind同步onedrive）<br>同步类型：双向<br>远程文件夹：/XMind/  （云上自建的）<br>本地文件夹：安卓端本地储存中的 XMind -&gt; workbook 文件夹<br>保存OK。（计划设置可自行按需设置。）</p>
<hr>
<p>参考：<br>1、<a href="https://support.xmind.net/hc/zh-cn/articles/360028527611" target="_blank" rel="noopener">如何将 XMind 文件从电脑上传输至 XMind 安卓版？</a><br>2、<a href="http://help.jianguoyun.com/?p=2887" target="_blank" rel="noopener">如何使用FolderSync在安卓手机上同步文件夹到坚果云？</a></p>
]]></content>
      <categories>
        <category>Software</category>
      </categories>
      <tags>
        <tag>XMind</tag>
        <tag>FolderSync</tag>
        <tag>OneDirve</tag>
      </tags>
  </entry>
  <entry>
    <title>PAPER READING&quot;2020-Zhou-Cross-Scale Internal Graph Neural Network for Image Super-Resolution&quot;</title>
    <url>/2021/01/29/zhouCrossScaleInternalGraph2020/</url>
    <content><![CDATA[<h1 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Zhou, Shangchen, Jiawei Zhang, Wangmeng Zuo, and Chen Change Loy. </span><br><span class="line">“Cross-Scale Internal Graph Neural Network <span class="keyword">for</span> Image Super-Resolution.”</span><br><span class="line">NeurIPS. virtual, 2020. </span><br><span class="line">https://proceedings.neurips.cc/paper/2020/<span class="built_in">hash</span>/23ad3e314e2a2b43b4c720507cec0723-Abstract.html.</span><br></pre></td></tr></table></figure>
<p>题目：用于图像超分的 交叉尺度 内部 图神经网络<br>NeurIPS. 2020<br>NTU MMLab PHD 周尚晨, 导师：吕健勤（Chen Change Loy）2016届董超的联合导师<br>个人web：<a href="https://shangchenzhou.com/" target="_blank" rel="noopener">Zhou, Shangchen</a>, <a href="http://personal.ie.cuhk.edu.hk/~ccloy/" target="_blank" rel="noopener">Chen Change Loy</a>.<br><a href="https://github.com/sczhou/IGNN" target="_blank" rel="noopener">[Code]</a> pytorch版</p>
<a id="more"></a>
<h1 id="METHODOLOGY"><a href="#METHODOLOGY" class="headerlink" title="METHODOLOGY"></a>METHODOLOGY</h1><p>核心思想如图<br><img src="https://i.loli.net/2021/01/30/ghzVkGDL65HcfW2.png" alt="quicker_1a8c4dde-7140-4166-9845-619c8a251e1a.png"></p>
<p>系统图<br><img src="https://i.loli.net/2021/01/30/1SIc7RdlUn6h4JQ.png" alt="quicker_dc08e518-1f82-4dd6-adc1-ba2ada5ba1ca.png"></p>
<h1 id="实验结果与分析"><a href="#实验结果与分析" class="headerlink" title="实验结果与分析"></a>实验结果与分析</h1><p>性能对比<br><img src="https://i.loli.net/2021/01/29/EHAb9xgnCWjqmQv.png" alt="quicker_e7b8893e-df50-4b7f-9a60-f6b92b4f8457.png"></p>
<p>运行时间 （补充材料）<br><img src="https://i.loli.net/2021/01/29/5OuBA4weNhUTxL7.png" alt="quicker_4cf6300b-750a-43aa-8b8c-73fc96bdb5f6.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>四问</p>
<pre><code>1. 作者想要完成什么（已经完成了什么）？
</code></pre><p>通过图卷积网络，提出了一种基于交叉（缩放）尺度的自相关性SISR方法。是即插即用的。该文在EDSR基础上实现，效果上得到提升，计算时间上相比EDSR翻倍，但比另外的方法效率高。</p>
<pre><code>2. 新方法的关键元素是什么？
</code></pre><p>Different from previous non-local methods that explore and aggregate neighboring patches at the same scale, we search for similar patches at the downsampled LR scale but aggregate HR patches. It allows our network to perform more efficiently and effectively for SISR.</p>
<pre><code>3. 对我有用吗？
</code></pre><p>用处不大。理论上可用于盲SR。但该文未尝试。<br>讨论过程再看看。</p>
<pre><code>4. 想看他的哪些参考文献？
</code></pre><p>对比方法中，2019年的几个引用。maybe</p>
<hr>
]]></content>
      <categories>
        <category>Paper reading</category>
        <category>Super resolution</category>
      </categories>
      <tags>
        <tag>Super resolution</tag>
        <tag>GNN</tag>
        <tag>non-local</tag>
      </tags>
  </entry>
</search>
